<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload="this.media='all'"><meta name=author content="Jeremiah Coholich"><meta name=description content="We combine three foundation models -- SAM, Grounded DINO, and FoundationPose -- for the purpose of running pose estimation on raw RGBD robot demonstrations. In order to improve results, we add a temporal consistency scoring function and various annotations to the data. Ultimately, the results are mixed and perhaps not good enough to provide object states to downstream robot planners."><link rel=alternate hreflang=en-us href=https://jmcoholich.github.io/post/foundationpose/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload="this.media='all'"><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload="this.media='all'"><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.95d53890c5839471cef2375b6c449b9f.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-QYSG8SXEWT"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag('event','click',{event_category:'outbound',event_label:e,transport_type:'beacon',event_callback:function(){t!=='_blank'&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=='A'||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute('target'))}gtag('js',new Date),gtag('config','G-QYSG8SXEWT',{}),gtag('set',{cookie_flags:'SameSite=None;Secure'}),document.addEventListener('click',onClickCallback,!1)</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huc310f1ac1642ff51805cd109dc789c7d_32737_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huc310f1ac1642ff51805cd109dc789c7d_32737_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://jmcoholich.github.io/post/foundationpose/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Jeremiah Coholich"><meta property="og:url" content="https://jmcoholich.github.io/post/foundationpose/"><meta property="og:title" content="FoundationPose for Robotics | Jeremiah Coholich"><meta property="og:description" content="We combine three foundation models -- SAM, Grounded DINO, and FoundationPose -- for the purpose of running pose estimation on raw RGBD robot demonstrations. In order to improve results, we add a temporal consistency scoring function and various annotations to the data. Ultimately, the results are mixed and perhaps not good enough to provide object states to downstream robot planners."><meta property="og:image" content="https://jmcoholich.github.io/post/foundationpose/featured.png"><meta property="twitter:image" content="https://jmcoholich.github.io/post/foundationpose/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2025-06-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-06-29T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jmcoholich.github.io/post/foundationpose/"},"headline":"FoundationPose for Robotics","image":["https://jmcoholich.github.io/post/foundationpose/featured.png"],"datePublished":"2025-06-29T00:00:00Z","dateModified":"2025-06-29T00:00:00Z","author":{"@type":"Person","name":"Jeremiah Coholich"},"publisher":{"@type":"Organization","name":"Jeremiah Coholich","logo":{"@type":"ImageObject","url":"https://jmcoholich.github.io/media/icon_huc310f1ac1642ff51805cd109dc789c7d_32737_192x192_fill_lanczos_center_3.png"}},"description":"We combine three foundation models -- SAM, Grounded DINO, and FoundationPose -- for the purpose of running pose estimation on raw RGBD robot demonstrations. In order to improve results, we add a temporal consistency scoring function and various annotations to the data. Ultimately, the results are mixed and perhaps not good enough to provide object states to downstream robot planners."}</script><title>FoundationPose for Robotics | Jeremiah Coholich</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=f2fa789e1a78128078b875dd1da0eb29><script src=/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Jeremiah Coholich</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Jeremiah Coholich</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#awards><span>Awards</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Blog</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>FoundationPose for Robotics</h1><p class=page-subtitle>My experience running FoundationPose and LangSAM for monocular RGBD object pose tracking in a tabletop maniulation setting</p><div class=article-metadata><div><span class=author-highlighted>Jeremiah Coholich</span></div><span class=article-date>Jun 29, 2025</span>
<span class=middot-divider></span>
<span class=article-reading-time>10 min read</span></div></div><div class="article-header container featured-image-wrapper mt-4 mb-4" style=max-width:1200px;max-height:675px><div style=position:relative><img src=/post/foundationpose/featured_huab63e3aa444d6cd3e178c4bec3d0dd8e_675078_1200x2500_fit_q75_h2_lanczos_3.webp width=1200 height=675 alt class=featured-image>
<span class=article-header-caption>Our tabletop manipulation setup with a Franka arm and FoundationPose tracking of plates</span></div></div><div class=article-container><div class=article-style><p>This blog post is about my experience using <a href=https://nvlabs.github.io/FoundationPose/ target=_blank rel=noopener>FoundationPose</a> with <a href=https://github.com/luca-medeiros/lang-segment-anything target=_blank rel=noopener>LangSAM</a>. It is meant to provide advice for others and third-party results for reference.</p><p><strong>TLDR;</strong> FoundationPose is generally not good enough to provide ground-truth object poses for real-world robot manipulation tasks. The model works somewhat off-the-shelf, but struggles significantly with small objects and occlusion. The model/code has no built-in way of dealing with objects going out-of-frame or complete occlusion, which is a big practical limitation. Also, see the <a href=#conclusion>Conclusion</a>.</p><h1 id=foundationpose-overview>FoundationPose Overview</h1><p>FoundationPose is a 6D object pose estimation model. It is trained on synthetically-augmented <a href=https://objaverse.allenai.org/ target=_blank rel=noopener>Objaverse</a> objects. At inference time, the model generates multiple pose hypotheses, ranks them, and outputs the rank 0 pose estimate. Unlike previous works, FoundationPose does not need to build a NeRF of the object first.</p><p>FoundationPose operates either in a model-based or model-free mode, where &ldquo;model&rdquo; refers to a CAD model (3D mesh) of the tracked object. To run FoundationPose model-free, several reference images of the object need to be supplied. We only tried the model-based version of FoundationPose.</p><p>For more details, see the <a href=https://arxiv.org/abs/2312.08344 target=_blank rel=noopener>paper</a>, however an in-depth understanding of FoundationPose is not required to use the model. Below is an overview of their method (Figure 2 from the paper).</p><figure id=figure-foundationpose-method-overview-source-httpsnvlabsgithubiofoundationpose><div class="d-flex justify-content-center"><div class=w-100><img alt="FoundationPose method overview (Source: https://nvlabs.github.io/FoundationPose/)" srcset="/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_8d0d7a45685e0683d002ab6dde0be991.webp 400w,
/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_25d3bae26eff8106401da62af9a4c926.webp 760w,
/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_8d0d7a45685e0683d002ab6dde0be991.webp width=760 height=488 loading=lazy data-zoomable></div></div><figcaption>FoundationPose method overview (Source: <a href=https://nvlabs.github.io/FoundationPose/ target=_blank rel=noopener>https://nvlabs.github.io/FoundationPose/</a>)</figcaption></figure><h1 id=langsam-overview>LangSAM Overview</h1><p>LangSAM is not a new method or architecture, but actually just code which combines the <a href=https://ai.meta.com/sam2/ target=_blank rel=noopener>Segment Anything</a> (SAM) model from Meta with the <a href=https://arxiv.org/abs/2303.05499 target=_blank rel=noopener>Grounding DINO</a> open-world object detector. Here, an understanding of what is really going on is helpful for effectively using and modifying LangSAM.</p><p>SAM is a powerful segmentation model that can generate pixelwise segmentations for anything in an image (as the name implies). SAM takes an image and a prompt and outputs a segmentation mask. The prompt can either be points, a bounding box, or a text string. However, Meta has not released a version of SAM with text conditioning. Fortunately, this capabitily can be reproduced by adding Grounding DINO.</p><p><figure id=figure-an-overview-of-the-segment-anything-model-source-httpsarxivorgabs230402643><div class="d-flex justify-content-center"><div class=w-100><img alt="An overview of the Segment Anything model (Source: https://arxiv.org/abs/2304.02643)" srcset="/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_c653aac0753229e743381931be70b682.webp 400w,
/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_f1a21f4b73641caa8ff8b40d2d93eabb.webp 760w,
/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_c653aac0753229e743381931be70b682.webp width=760 height=155 loading=lazy data-zoomable></div></div><figcaption>An overview of the Segment Anything model (Source: <a href=https://arxiv.org/abs/2304.02643 target=_blank rel=noopener>https://arxiv.org/abs/2304.02643</a>)</figcaption></figure><figure id=figure-example-images-segmented-by-sam-containing-400-to-500-masks-per-image-source-httpsarxivorgabs230402643><div class="d-flex justify-content-center"><div class=w-100><img alt="Example images segmented by SAM containing 400 to 500 masks per image (Source: https://arxiv.org/abs/2304.02643)" srcset="/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_8d4564c3de56d3457bd2ee272db29380.webp 400w,
/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_8cc8c342f01e7aabae97e72b716d7714.webp 760w,
/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_8d4564c3de56d3457bd2ee272db29380.webp width=760 height=136 loading=lazy data-zoomable></div></div><figcaption>Example images segmented by SAM containing 400 to 500 masks per image (Source: <a href=https://arxiv.org/abs/2304.02643 target=_blank rel=noopener>https://arxiv.org/abs/2304.02643</a>)</figcaption></figure></p><p><a href=https://arxiv.org/abs/2303.05499 target=_blank rel=noopener>Grounding DINO</a> is an open-world object detector that takes a string of text and outputs bounding box proposals. It was created by fusing a closed-set object detector, <a href=https://arxiv.org/abs/2203.03605 target=_blank rel=noopener>DINO</a>, with a text encoder, <a href=https://arxiv.org/abs/1810.04805 target=_blank rel=noopener>BERT</a>. LangSAM takes the bounding box proposals from Grounding DINO and feeds them into SAM to obtain a pixel-wise segmentation mask. This <a href=https://lightning.ai/blog/lang-segment-anything-object-detection-and-segmentation-with-text-prompt target=_blank rel=noopener>blog post</a> explains LangSAM in much more detail.</p><p>We have two reasons for using LangSAM:</p><ul><li>FoundationPose requires a segmentation mask of the tracked object(s) in the first frame to initialize pose estimation.</li><li>We want a database of object segmentations for a data-augmentation task not covered in this article.</li></ul><h1 id=off-the-shelf-performance>&ldquo;Off-the-shelf&rdquo; Performance</h1><p>FoundationPose requires RGBD video frames, a CAD model, camera intrinsics, and a binary segmentation mask of the tracked object in the first frame.</p><p>Below is a visualization of our input video. Its is a VR-teleoperated demonstration of a block-stacking task.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/8bc508QxUwo style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>The three views are captured with RealSense D435 cameras running at 1280x720 resolution. The intrisic matrix is:</p><p>$$ K = \begin{bmatrix} 912.0 & 0.0 & 640.0 \\ 0.0 & 912.0 & 360.0 \\ 0.0 & 0.0 & 1.0 \end{bmatrix} $$</p><p>The cups, blocks, and plates we used for experiments were purchased on Amazon:</p><ul><li>Cups: <a href=https://a.co/d/9PSu2UX target=_blank rel=noopener>https://a.co/d/9PSu2UX</a></li><li>Blocks (painted after purchasing): <a href=https://a.co/d/i5k3pBq target=_blank rel=noopener>https://a.co/d/i5k3pBq</a></li><li>Plates: <a href=https://a.co/d/6hOiS2a target=_blank rel=noopener>https://a.co/d/6hOiS2a</a></li></ul><p>(These are not affiliate links; I&rsquo;m not trying to sell anything. These are simply the objects that correspond to the CAD models I made.)</p><p>The CAD models I created for each of them are available <a href=https://github.com/jmcoholich/FoundationPose/tree/main/meshes target=_blank rel=noopener>here</a>.</p><p>To output 6D object poses from a video frame, FoundationPose first requires an initial for both translation and rotation. The initial guess is then refined for a set number of iterations (default is 5) before a final estimate is output. For every frame except the first one, the pose estimate of the previous frame is used for initialization. When processing the first frame, FoundationPose generate 240 initial rotation guesses through sampling points and rotations on an <a href=https://en.wikipedia.org/wiki/Geodesic_polyhedron target=_blank rel=noopener>icosphere</a>. The first-frame initial guess for translation is simply obtained through the segmentation mask.</p><p>I modified the FoundationPose code to track all three cubes at once with a simple &ldquo;for&rdquo; loop. Below are the first results. I&rsquo;m only running tracking on the front camera view.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/CTuzFU3Y9gI style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>Clearly there are some issues &ndash; the model is unable to track the blocks once they are moved.</p><h1 id=foundationpose--mask-temporal-consistency>FoundationPose + Mask Temporal Consistency</h1><p>My first idea for improving these results was to condition the pose estimate for every frame on a segmentation masks from LangSAM (instead of just the first frame). Essentially, this offloads the challenge of object localization in 2D from FoundationPose to LangSAM. However, LangSAM doesn&rsquo;t work perfectly either. Using the same prompts for the same objects on every frame (&ldquo;blue cube&rdquo;, &ldquo;red cube&rdquo;, and &ldquo;green cube&rdquo;), this is what we get:</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/2YxygrxXshY style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>Watch the top right view. When the red cube is manipulated, LangSAM switches to segmenting the blue cube, then the green cube, and eventually the entire stack of cubes. However, all the segmentations in the first frame are correct, likely because none of the cubes are occluded by the gripper or stacked.</p><p>As explained previously, bounding boxes from Grounding DINO are used to prompt SAM. For each frame, Grounding DINO outputs multiple box proposals that are above a settable threshold for bounding-box validity and text alignment then selects the box with the highest text alignment. Below is an image of the bounding box proposals and scores for the first frame for the prompt &ldquo;red cube&rdquo;. The highest scoring box is colored blue</p><figure id=figure-bounding-box-proposals--and-alignment-scores-for-prompt-red-cube-from-grounding-dino><div class="d-flex justify-content-center"><div class=w-100><img alt='Bounding box proposals  and alignment scores for prompt "red cube" from Grounding DINO' srcset="/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_7dc6b7d8b1ddc044b3ab9cb2a14a9ad8.webp 400w,
/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_a17385909cf543f39efbccdd02b8c98e.webp 760w,
/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_7dc6b7d8b1ddc044b3ab9cb2a14a9ad8.webp width=760 height=428 loading=lazy data-zoomable></div></div><figcaption>Bounding box proposals and alignment scores for prompt &ldquo;red cube&rdquo; from Grounding DINO</figcaption></figure><p>I created a new scoring function to reward consistency with the previous frame. This scoring function is used instead of the prompt-alignment score for all frames except for the first one. This function is:</p><p>$$
x_t = \arg\min_{\mathbf{x}} \left|\mathbf{x}_{t - 1} - \mathbf{x}\right|_1
$$</p><p>Where $\mathbf{x}$ is a vector representing a bounding box.</p><p>Or with Pytorch:</p><pre style=font-size:16px;color:#11b321;background-color:#000>
dist_score = -torch.sum(torch.abs(bbox - last_bbox))
</pre><p>We also lower the &ldquo;box_threshold&rdquo; and &ldquo;text_threshold&rdquo; from 0.3 and 0.25 to 0.1 and 0.1. This means Grounded DINO will output more bounding box proposals for us to select from.</p><p>Here is the second frame, with the temporal consistency scores displayed:<figure id=figure-bounding-box-proposals-and-temporal-consistency-scores-for-prompt-red-cube-from-grounding-dino><div class="d-flex justify-content-center"><div class=w-100><img alt='Bounding box proposals and temporal consistency scores for prompt "red cube" from Grounding DINO' srcset="/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_8a22573388f1d3816a51a2014b20a79a.webp 400w,
/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_45285f9935203566b35bedcfe8d5355a.webp 760w,
/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_8a22573388f1d3816a51a2014b20a79a.webp width=760 height=428 loading=lazy data-zoomable></div></div><figcaption>Bounding box proposals and temporal consistency scores for prompt &ldquo;red cube&rdquo; from Grounding DINO</figcaption></figure></p><p>Here is another example from the side camera view (145th frame):<figure id=figure-bounding-box-proposals-and-temporal-consistency-scores-for-prompt-red-cube-from-grounding-dino-with-many-bounding-box-proposals><div class="d-flex justify-content-center"><div class=w-100><img alt='Bounding box proposals and temporal consistency scores for prompt "red cube" from Grounding DINO, with many bounding box proposals' srcset="/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_f96dd70071f7f3975af0d0a7497d61e5.webp 400w,
/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_ca27947d5e14d2ce86e1762964aae5de.webp 760w,
/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_1200x1200_fit_q75_h2_lanczos.webp 1200w" src=/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_f96dd70071f7f3975af0d0a7497d61e5.webp width=760 height=428 loading=lazy data-zoomable></div></div><figcaption>Bounding box proposals and temporal consistency scores for prompt &ldquo;red cube&rdquo; from Grounding DINO, with many bounding box proposals</figcaption></figure></p><p>Below is are the segmentations for the same demo obtained with the temporal-consistency scoring function.<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/i0zaNuNY9RM style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div>Clearly, the tracking is much better now. Watching the top right view again and observe that the red block is segmented correctly the entire video, even during manipulation and stacking. However, there are still some small errors. For example, for cam 0 &ldquo;Franka robot arm&rdquo;, the block is segmented instead of the robot (the tip of the gripper). This will be addressed in the next section.</p><p>Below is the video of the FoundationPose results where every frame is conditioned on the improved, temporally-consistency segmentations from LangSAM.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/NemeM3IC1gU style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><p>Now, the model is able to track each block throughout the demo.</p><p>You may also notice that the coordinate systems for each box are now aligned, in contrast to the first FoundationPose video. I realized that since cubes have many axes of symmetry, the icosphere of pose initializations lead to random cube orientations. I reduced the number of initial guesses to a single identity matrix which results in consistent cube pose estimates and a ~150% speedup of FoundationPose.</p><h1 id=foundationpose--labels>FoundationPose + Labels</h1><p>Occasionally, LangSAM segmentations are wrong even for the first frame, meaning the temporal consistency score only enforces an incorrect object segmentation. Additionally, sometimes the objects are occluded completely or leave the image entirely, making tracking impossible. Since we were shooting for a deadline and needed results quickly, I decided to manually add some labels to our collection of 60 demonstrations. With the help of ChatGPT, I wrote a labeling pipeline to step through each video and provide labels for object bounding boxes and &ldquo;stop tracking&rdquo; for select frames in the video.</p><p>Here are the segemtations for the same stack-blocks demo again. Now, the cam 0 &ldquo;Franka robot arm&rdquo; segmentation is correct, even in the first frame.</p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/TDYzbGJ2REg style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div><h3 id=stack-cups-and-stack-plates-tasks>Stack Cups and Stack Plates Tasks</h3><p>With the stack-blocks task working, I ran the pipeline on two new tasks &ndash; stack-cups and stack-plates.</p><p>Here is FoundationPose on stack-plates without temporal consistency and labels:</p><p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/rUjEtP8KPmw style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div>Tracking of the plates fails when they are lifted out of the scene and when they occlude each other on the rack.</p><p>Here are the results after adding temporal consistency and labels:</p><p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/ilF_YeErRAM style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div>The labels that tell FoundationPose when to stop and reinitialize tracking significantly improves the pose estimates.</p><p>Here is the before and after for the stack-cups task.<div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/3QjOZKr2tlg style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div></p><p><div style=position:relative;padding-bottom:56.25%;height:0;overflow:hidden><iframe src=https://www.youtube.com/embed/G7xddmpxsf4 style=position:absolute;top:0;left:0;width:100%;height:100%;border:0 allowfullscreen title="YouTube Video"></iframe></div>Much less of a difference is made here, partially because the tracking worked well off-the-shelf, for some reason. I don&rsquo;t know why the model succeeded here and failed on the cubes, since both tasks are quite similar and use similarly-sized objects. Perhaps FoundationPose training dataset contained more objects like the cups.</p><h1 id=conclusion>Conclusion</h1><p>Temporal consistency scoring greatly improved results for the stack-blocks task, while data annotations greatly improved results for the stack-plates task. However, the results are still far from perfrect and providing high-quality annotations for demo videos tracking multiple objects is not scalable.</p><p>For now, we&rsquo;ve stopped using FoundationPose and are looking for other solutions. I talked to two other PhD students working on manipulation who had also tried to use FoundationPose and abandoned it, saying my video results were better than theirs even. However, for those interested in trying, here are some recommendations.</p><h3 id=my-recommendations>My Recommendations</h3><p>For real-world object pose tracking, the best thing to use is motion capture. The second best thing would be to use several <a href=https://april.eecs.umich.edu/software/apriltag target=_blank rel=noopener>AprilTags</a> on each object. If that is not an option either, then use FoundationPose or BundleSDF. To obtain decent results with these models:</p><ul><li>Track large objects or move the cameras closer to the scene</li><li>Use the highest camera resolution</li><li>Avoid occluding the tracked objects</li><li>Avoid moving the tracked objects out of frame</li><li>If you have a higher compute budget, try <a href=https://bundlesdf.github.io/ target=_blank rel=noopener>BundleSDF</a>. BundleSDF takes longer to run for each video, but handles occlusions much better.</li></ul><p>For the LangSAM language-to-segmentation pipeline, I recommend:</p><ul><li>Choosing objects that are distinct from other objects in the scene in terms of shape and color</li><li>Spending time experimenting with different prompts</li><li>Using an LLM to generate prompts to try</li><li>Use bounding box prompting for SAM (instead of points)</li></ul><p>Here is my Fork of FoundationPose: <a href=https://github.com/jmcoholich/FoundationPose target=_blank rel=noopener>https://github.com/jmcoholich/FoundationPose</a>.
Tips for running FoundationPose from the authors: <a href=https://github.com/NVlabs/FoundationPose/issues/44#issuecomment-2048141043 target=_blank rel=noopener>https://github.com/NVlabs/FoundationPose/issues/44#issuecomment-2048141043</a>
<a href=https://github.com/030422Lee/FoundationPose_manual target=_blank rel=noopener>https://github.com/030422Lee/FoundationPose_manual</a></p><p>A huge thanks to the FoundationPose authors for developing and releasing this model! Also thank you to <a href=https://www.linkedin.com/in/justin-wit/ target=_blank rel=noopener>Justin Wit</a> for helping me setup tasks and collect data for all experiments shown.</p><h2 id=references>References</h2><div style=font-size:12px><blockquote style="margin:.3em 0">Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding." In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 4171-4186. 2019.</blockquote><blockquote style="margin:.3em 0">Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao et al. "Segment anything." In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4015-4026. 2023.</blockquote><blockquote style="margin:.3em 0">Liu, Shilong, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang et al. "Grounding dino: Marrying dino with grounded pre-training for open-set object detection." In European Conference on Computer Vision, pp. 38-55. Cham: Springer Nature Switzerland, 2024.</blockquote><blockquote style="margin:.3em 0">Medeiros, Luca. 2023. lang-segment-anything. GitHub. https://github.com/luca-medeiros/lang-segment-anything.</blockquote><blockquote style="margin:.3em 0">Ravi, Nikhila, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr et al. "Sam 2: Segment anything in images and videos." arXiv preprint arXiv:2408.00714 (2024).</blockquote><blockquote style="margin:.3em 0">Wen, Bowen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Müller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. "Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 606-617. 2023.</blockquote><blockquote style="margin:.3em 0">Wen, Bowen, Wei Yang, Jan Kautz, and Stan Birchfield. "Foundationpose: Unified 6d pose estimation and tracking of novel objects." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17868-17879. 2024.</blockquote><blockquote style="margin:.3em 0">Zhang, Hao, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum. "Dino: Detr with improved denoising anchor boxes for end-to-end object detection." arXiv preprint arXiv:2203.03605 (2022).</blockquote></div></div><div class=share-box><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://jmcoholich.github.io/post/foundationpose/&text=FoundationPose%20for%20Robotics" target=_blank rel=noopener class=share-btn-twitter aria-label=twitter><i class="fab fa-twitter"></i></a></li><li><a href="mailto:?subject=FoundationPose%20for%20Robotics&body=https://jmcoholich.github.io/post/foundationpose/" target=_blank rel=noopener class=share-btn-email aria-label=envelope><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://jmcoholich.github.io/post/foundationpose/&title=FoundationPose%20for%20Robotics" target=_blank rel=noopener class=share-btn-linkedin aria-label=linkedin-in><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=https://jmcoholich.github.io/post/foundationpose/&title=FoundationPose%20for%20Robotics" target=_blank rel=noopener class=share-btn-reddit aria-label=reddit-alien><i class="fab fa-reddit-alien"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href=https://jmcoholich.github.io><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hue9a825943fcb1638935d181430d198c4_664807_270x270_fill_q75_lanczos_center.jpeg alt="Jeremiah Coholich"></a><div class=media-body><h5 class=card-title><a href=https://jmcoholich.github.io>Jeremiah Coholich</a></h5><h6 class=card-subtitle>Robotics PhD Student</h6><p class=card-text>My research interests include deep learning, reinforcement learning, and legged robots.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:jcoholich@gatech.edu><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?user=KXw8YxAAAAAJ&hl=en&oi=ao" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/jmcoholich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/jeremiah-coholich/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/uploads/CV_Fall_2024.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2025 Jeremiah Coholich</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.e66e385e2f1df861699d60acd7a9c670.js></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.ab2f2890dbe3e2e83579366d3d6e8fd9.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>