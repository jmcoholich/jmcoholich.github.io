<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.5.0 for Hugo"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media=print onload='this.media="all"'><meta name=author content="Jeremiah Coholich"><meta name=description content="Outline of this post
RL is a very exciting and promising field BUT its hard to reproduce results and hard to apply to new fields This blog post gives a list of tricks and lessons learned for beginners trying to write RL algorithms from scratch and/or apply RL algorithms to new tasks When I first started studying reinforcement learning (RL), I implemented Proximal Policy Optimization from scratch using only the psuedocode on OpenAI&rsquo;s website."><link rel=alternate hreflang=en-us href=https://jmcoholich.github.io/post/rl_lessons/><meta name=theme-color content="#1565c0"><script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin=anonymous media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload='this.media="all"'><link rel=stylesheet href=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload='this.media="all"' disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href=/css/wowchemy.95d53890c5839471cef2375b6c449b9f.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-QYSG8SXEWT"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}function trackOutboundLink(e,t){gtag("event","click",{event_category:"outbound",event_label:e,transport_type:"beacon",event_callback:function(){t!=="_blank"&&(document.location=e)}}),console.debug("Outbound link clicked: "+e)}function onClickCallback(e){if(e.target.tagName!=="A"||e.target.host===window.location.host)return;trackOutboundLink(e.target,e.target.getAttribute("target"))}gtag("js",new Date),gtag("config","G-QYSG8SXEWT",{}),gtag("set",{cookie_flags:"SameSite=None;Secure"}),document.addEventListener("click",onClickCallback,!1)</script><link rel=manifest href=/manifest.webmanifest><link rel=icon type=image/png href=/media/icon_huc310f1ac1642ff51805cd109dc789c7d_32737_32x32_fill_lanczos_center_3.png><link rel=apple-touch-icon type=image/png href=/media/icon_huc310f1ac1642ff51805cd109dc789c7d_32737_180x180_fill_lanczos_center_3.png><link rel=canonical href=https://jmcoholich.github.io/post/rl_lessons/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Jeremiah Coholich"><meta property="og:url" content="https://jmcoholich.github.io/post/rl_lessons/"><meta property="og:title" content="A Bag of Tricks for Deep Reinforcement Learning | Jeremiah Coholich"><meta property="og:description" content="Outline of this post
RL is a very exciting and promising field BUT its hard to reproduce results and hard to apply to new fields This blog post gives a list of tricks and lessons learned for beginners trying to write RL algorithms from scratch and/or apply RL algorithms to new tasks When I first started studying reinforcement learning (RL), I implemented Proximal Policy Optimization from scratch using only the psuedocode on OpenAI&rsquo;s website."><meta property="og:image" content="https://jmcoholich.github.io/post/rl_lessons/featured.png"><meta property="twitter:image" content="https://jmcoholich.github.io/post/rl_lessons/featured.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-05-31T00:00:00+00:00"><meta property="article:modified_time" content="2022-05-31T00:00:00+00:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://jmcoholich.github.io/post/rl_lessons/"},"headline":"A Bag of Tricks for Deep Reinforcement Learning","image":["https://jmcoholich.github.io/post/rl_lessons/featured.png"],"datePublished":"2022-05-31T00:00:00Z","dateModified":"2022-05-31T00:00:00Z","author":{"@type":"Person","name":"Jeremiah Coholich"},"publisher":{"@type":"Organization","name":"Jeremiah Coholich","logo":{"@type":"ImageObject","url":"https://jmcoholich.github.io/media/icon_huc310f1ac1642ff51805cd109dc789c7d_32737_192x192_fill_lanczos_center_3.png"}},"description":"Outline of this post\nRL is a very exciting and promising field BUT its hard to reproduce results and hard to apply to new fields This blog post gives a list of tricks and lessons learned for beginners trying to write RL algorithms from scratch and/or apply RL algorithms to new tasks When I first started studying reinforcement learning (RL), I implemented Proximal Policy Optimization from scratch using only the psuedocode on OpenAI\u0026rsquo;s website."}</script><title>A Bag of Tricks for Deep Reinforcement Learning | Jeremiah Coholich</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=05728d24d8c70cbd5c28a4cf5528c6e1><script src=/js/wowchemy-init.min.2ed908358299dd7ab553faae685c746c.js></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=# aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><header class=header--fixed><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>Jeremiah Coholich</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>Jeremiah Coholich</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#awards><span>Awards</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Blog</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class="nav-item dropdown theme-dropdown"><a href=# class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href=# class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href=# class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href=# class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></header></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>A Bag of Tricks for Deep Reinforcement Learning</h1><div class=article-metadata><div><span class=author-highlighted>Jeremiah Coholich</span></div><span class=article-date>May 31, 2022</span>
<span class=middot-divider></span>
<span class=article-reading-time>10 min read</span></div></div><div class="article-header container featured-image-wrapper mt-4 mb-4" style=max-width:1200px;max-height:803px><div style=position:relative><img src=/post/rl_lessons/featured_hu601c9f480be93834cf7280d8bffbd671_457153_1200x2500_fit_q75_h2_lanczos_3.webp width=1200 height=803 alt class=featured-image>
<span class=article-header-caption>The KUKA bin environment visualized in NVIDIA IsaacGym</span></div></div><div class=article-container><div class=article-style><p>Outline of this post</p><ol><li>RL is a very exciting and promising field</li><li>BUT its hard to reproduce results and hard to apply to new fields</li><li>This blog post gives a list of tricks and lessons learned for beginners trying to write RL algorithms from scratch and/or apply RL algorithms to new tasks</li></ol><p>When I first started studying reinforcement learning (RL), I implemented <a href=https://arxiv.org/abs/1707.06347 target=_blank rel=noopener>Proximal Policy Optimization</a> from scratch using only the <a href=https://spinningup.openai.com/en/latest/algorithms/ppo.html#pseudocode target=_blank rel=noopener>psuedocode</a> on OpenAI&rsquo;s website. It didn&rsquo;t work and failed to obtain nearly any reward on most OpenAI Gym environments. It took a few more months of debugging, reading other RL implementations, and talking to colleagues to get things working. My conversations with other Georgia Tech students revealed that initially struggling to do basic things with RL was not uncommon. <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html#:~:text=Often%2C%20it%20doesn%27t%2C,out%20of%20the%20RL%20algorithm." target=_blank rel=noopener>These</a> blog <a href=https://andyljones.com/posts/rl-debugging.html target=_blank rel=noopener>posts</a> do a great job of explaining the difficulty with RL and really resonate with my own experiences.</p><p>In hindsight, there was no single major flaw with my initial PPO implementation, but rather many small tricks and optimizations that were missing. The purpose of this post is to enumerate these tricks and provide references to code where they are implemented. Some of these things you really only need to worry about if you decide to write an RL algorithm from scratch, as most implementations will already include them. However, knowing of their existence will enable you to debug more effictively and make changes more intelligently. They are roughly ordered in descending order of importance.</p><p>Different RL implementations will include a slightly different set of tricks. As evidence of their importance, check out this figure (below) from <a href=https://arxiv.org/pdf/1709.06560.pdf target=_blank rel=noopener>Deep RL that matters</a>. The authors show empirically that different popular implementions of the same RL algorithm differ significantly in performance on standard RL benchmarks, even when controlling for hyperparameters and network architecture.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=pic srcset="/post/rl_lessons/fig_6_drl_that_matters_hu1a33ed80bb1b77cc501b4b0193998bdb_219395_7a4e3091472c5b5c370db2e33c1ba318.webp 400w,
/post/rl_lessons/fig_6_drl_that_matters_hu1a33ed80bb1b77cc501b4b0193998bdb_219395_026bbb84274df387aede9a3e32603934.webp 760w,
/post/rl_lessons/fig_6_drl_that_matters_hu1a33ed80bb1b77cc501b4b0193998bdb_219395_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/rl_lessons/fig_6_drl_that_matters_hu1a33ed80bb1b77cc501b4b0193998bdb_219395_7a4e3091472c5b5c370db2e33c1ba318.webp width=693 height=760 loading=lazy data-zoomable></div></div></figure><em>Figure 6 from <a href=https://arxiv.org/pdf/1709.06560.pdf target=_blank rel=noopener>Deep Reinforcement Learning that Matters</a> plotting the performance of different RL implementations, averaged over 5 random seeds</em></p><p>Now for some disclaimers: Nearly all of my experience comes from training on-policy algorithms for continuous control, so there may be useful tips for discrete/off-policy settings that I am missing. Also, RL is a super-hot field, so perhaps some of the content in this post is already outdated. Hopefully, this blog is at least useful to someone starting out like I was. Please don&rsquo;t hesitate to reach out to me if you think there is something important missing!</p><p>Most of the examples will come from either of these two RL implementations of which I am familair with:
<a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail target=_blank rel=noopener>https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail</a>
<a href=https://github.com/Denys88/rl_games target=_blank rel=noopener>https://github.com/Denys88/rl_games</a></p><p>Implementing an RL algorithm from scratch is an excellent way to learn. However, if you need to get something working quickly, you should instead just fork a popular repo. Here are some suggestions:</p><ul><li><a href=https://stable-baselines3.readthedocs.io/en/master/ target=_blank rel=noopener>Stable Baselines3</a></li><li><a href=https://github.com/Denys88/rl_games target=_blank rel=noopener>RL Games</a></li><li><a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail target=_blank rel=noopener>pytorch-a2c-ppo-acktr</a></li><li><a href=https://docs.ray.io/en/latest/rllib/index.html target=_blank rel=noopener>RLlib</a></li></ul><p>Thanks to Elon Musk for helping me proofread and edit this post.</p><p>TODO post this on the RL discord.</p><h3 id=observation-normalization-and-clipping>Observation Normalization and Clipping</h3><p>In RL, the inputs to the policy and value networks are observations, which can consist of values that differ by orders of magnitude. For example, if you are learning a policy to control a robot, your observation could contain joint angles ranging from $ -\frac{\pi}{2} $ to $ \frac{\pi}{2} $ radians and a robot position coordinate that lies between 0 and 1000 meters. Normalizing the input space to eliminate this difference in scale leads to more stable training and faster convergence. This should be nothing new to those with prior experience training neural networks.</p><p>The two most common methods for preprocessing are standardization and rescaling. Standardization refers to subtracting the mean and dividing by the standard deviation of the data so that each dimension approximates a standard normal distribution. Rescaling refers to mapping the data to the range $ \left[0, 1\right] $ by subtacting the min and dividing by the range.</p><p>In supervised learning, statistics calculated over the training set are used to normalize each sample. In RL, this isn&rsquo;t possible because the dataset (consisting of interactions with the environment) is collected online and the statistics change continuously. Because of this, you need to calculate an online mean and standard deviation. Most RL codebases use an implementation of <a href=https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford%27s_online_algorithm target=_blank rel=noopener>Welford&rsquo;s Online Algorithm</a> like <a href=https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/running_mean_std.py target=_blank rel=noopener>this one</a> from Stable Baselines3.</p><p>This online approach is best when your algorithm needs to work on many different environments. Typically, this style of input normalization causes an initial drop in performance as the mean and standard deviation move very early in training due a small sample size and rapid exploration.</p><p><figure><div class="d-flex justify-content-center"><div class=w-100><img alt=pic srcset="/post/rl_lessons/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_cb7351224d24cbd48db808cd6bb44e05.webp 400w,
/post/rl_lessons/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_f3c994e5b8222cfd49a9dd4f22fd932e.webp 760w,
/post/rl_lessons/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/rl_lessons/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_cb7351224d24cbd48db808cd6bb44e05.webp width=760 height=380 loading=lazy data-zoomable></div></div></figure></p><p>Alternatively, if you have good prior knowledge about the observations space, you can just rescale your data to the range [-1, 1] or [0, 1], like what they do <a href=https://github.com/leggedrobotics/legged_gym/blob/dd6a6892e54c4f111a203319c05da8dca9595ae1/legged_gym/envs/base/legged_robot.py#L212 target=_blank rel=noopener>here</a>.</p><p><strong>Note:</strong> A common bug when replaying trained policies is the failure to save and load normalization statistics. A policy network will not work during test time if the inputs are not preprocessed the same way they were during training.</p><p>Code examples</p><ul><li><a href=https://github.com/Denys88/rl_games/blob/06a3319d3a6af566d984aa5953b1fd7a24a8e3a4/rl_games/common/a2c_common.py#L587 target=_blank rel=noopener>https://github.com/Denys88/rl_games/blob/06a3319d3a6af566d984aa5953b1fd7a24a8e3a4/rl_games/common/a2c_common.py#L587</a></li><li><a href=https://github.com/Denys88/rl_games/blob/94e55563be60f10e659428cdce7b4e0bd131d471/rl_games/algos_torch/models.py#L41 target=_blank rel=noopener>https://github.com/Denys88/rl_games/blob/94e55563be60f10e659428cdce7b4e0bd131d471/rl_games/algos_torch/models.py#L41</a></li></ul><h3 id=a-dense-reward-function>A dense reward function</h3><p>(dense = every timestep, smooth = varies smoothly between regions of the state space (ie gradual change vs large steps))</p><p>This tip will only be applicable if you are applying RL to a new task where you have the freedom to specify a reward function, rather than training on standard RL benchmarks where the reward function is part of the task.</p><p>Sparse rewards are difficult for RL algorithms to learn from. If possible, try making your reward <em>dense</em>, meaning that at every timestep the agent recieves an informantive reward as a function of the current state, previous state, and action taken by the policy. For example, instead of rewarding an agent +1.0 for reaching a goal and 0.0 otherwise, try giving a reward at every timestep that is propotional to progress towards the goal. Of course, this requires some prior knowledge of what progress looks like and can limit what your policy discovers accordingly.</p><p>For example, in the paper <a href=https://arxiv.org/abs/2005.04323 target=_blank rel=noopener>ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills
</a>, the authors train a bipedal robot to hit a series of stepping stones. A naive reward design would give +1.0 if the robot&rsquo;s foot hit the center of the foot target, and 0.0 otherwise. Instead of doing this, the authors specify a reward function of</p>$$ r_{target} = k_{target}exp(-d/k_d) $$<p>where d is the distance from the foot to the target, and $ k_{target}$ and $k_d$ are hyperparameters. The authors explain:</p><blockquote><p>In the initial stages of training, when the character makes contact with the target, the contact location may be far away from the center. Consequently, the gradient with respect to the target reward is large due to the exponential, which encourages the policy to move the foot closer to the center in the subsequent training iterations.</p></blockquote><p>Additionally, the authors reward robot center-of-mass velocity towards the next footsteps which further provides a progress signal.</p><h3 id=gradient-normalization-and-clipping>Gradient Normalization and Clipping</h3><p>This is another one that could be obvious if you have a background in supervised learning. Normalizing the gradient of the value and policy networks after each backward pass can help avoid numerical overflow, exploding gradients, or destructively large parameter updates. Other tricks for avoiding these same issues include rewarding normalization and clipping, value function loss clipping, and advantage standardization.</p><p>Code examples:</p><ul><li><a href=https://github.com/Denys88/rl_games/blob/8da6852f72bdbe867bf12f792b00df944b419c43/rl_games/common/a2c_common.py#L252 target=_blank rel=noopener>https://github.com/Denys88/rl_games/blob/8da6852f72bdbe867bf12f792b00df944b419c43/rl_games/common/a2c_common.py#L252</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html target=_blank rel=noopener>torch.nn.utils.clip_grad_norm_</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html target=_blank rel=noopener>torch.nn.utils.clip_grad_value_</a></li></ul><h3 id=reward-normalization-and-clipping>Reward Normalization and Clipping</h3><p>Typically, it is best not to have reward values that differ by many orders of magnitude. For example, in the paper <a href=https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf target=_blank rel=noopener>Playing Atari with Deep Reinforcement Learning</a>, the authors clip all rewards to the range $ \left[-1, 1\right] $.</p><blockquote><p>Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude.<br>&ndash; <cite>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. &ldquo;Playing atari with deep reinforcement learning.&rdquo; arXiv preprint arXiv:1312.5602 (2013).</cite></p></blockquote><p>In addition to just clipping the rewards, you can also keep a running mean and standard deviations of rewards to standardize rewards or returns (discounted rewards).</p><h3 id=advantage-standardization>Advantage Standardization</h3><p>Before calculating a loss for the policy network, advantages are computed and then standardized, such that about half of the advantages are positive and about half are negative. This is done for stability of training and variance reduction. Here is an excert from <a href=http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/hw2_final.pdf target=_blank rel=noopener>HW2</a> of the Berkely Deep RL course:</p><blockquote><p>A trick which is known to usually boost empirical performance by lowering variance of the
estimator is to center advantages and normalize them to have mean of 0 and a standard
deviation of 1.
From a theoretical perspective, this does two things:</p><ul><li>Makes use of a constant baseline at all timesteps for all trajectories, which does not
change the policy gradient in expectation.</li><li>Rescales the learning rate by a factor of 1/σ, where σ is the standard dev of the
empirical advantages.</li></ul></blockquote><p>Code Examples</p><ul><li><a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/41332b78dfb50321c29bade65f9d244387f68a60/a2c_ppo_acktr/algo/ppo.py#L36 target=_blank rel=noopener>https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/41332b78dfb50321c29bade65f9d244387f68a60/a2c_ppo_acktr/algo/ppo.py#L36</a></li></ul><h3 id=hyperparameter-tuning>Hyperparameter Tuning</h3><p>RL is notoriously sensitive to hyperparameters and there is no one-size-fits all for good hyperparameter values. Typically, different implementations and different applications will need different hyperparameters. This is a major complaint about RL.</p><p>The good thing is, <a href=https://wandb.ai/site target=_blank rel=noopener>Weights & Biases</a> has a very good pipeline for doing automated, distributed hyperparameter sweeps. They support random search, grid search, and Bayesian search.
<a href=https://docs.wandb.ai/guides/sweeps target=_blank rel=noopener>Check it out.</a></p><h3 id=value-network-loss-clipping>Value Network Loss Clipping</h3><p>This is another trick aimed at controlling the behavior. The mean-squared error loss that the value function is trained on is clipped from [-k, k] where k is usually around 0.2.</p><p>Code Examples:</p><ul><li><a href=https://github.com/Denys88/rl_games/blob/8da6852f72bdbe867bf12f792b00df944b419c43/rl_games/common/common_losses.py#L7 target=_blank rel=noopener>https://github.com/Denys88/rl_games/blob/8da6852f72bdbe867bf12f792b00df944b419c43/rl_games/common/common_losses.py#L7</a></li><li><a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/41332b78dfb50321c29bade65f9d244387f68a60/a2c_ppo_acktr/algo/ppo.py#L68 target=_blank rel=noopener>https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/41332b78dfb50321c29bade65f9d244387f68a60/a2c_ppo_acktr/algo/ppo.py#L68</a></li></ul><h3 id=learning-rate-scheduling>Learning Rate Scheduling</h3><p>A commong thing is to implement a linearly decreasing learning rate throughout training. The idea is that towards the end of training, you want to avoid making descrutively large policy updates (this is also the idea behind TRPO and allocating your retirement savings into bonds as you grow older.) and your performance will have mostly saturated so you should just be fine-tuning your policy.</p><p>A fancier way to do this is to adaptively set the learning rate based on a desired <a href=https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence target=_blank rel=noopener>KL-divergence</a> between something. I have mostly used this for my work with the quadruped, set the initial learning rate to 1e-5 and then let the adaptive lr take over. Here is what the lr plot usually looks like.<figure><div class="d-flex justify-content-center"><div class=w-100><img alt=pic srcset="/post/rl_lessons/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_c1d09df98d6fcc1d90dd947d5349c4c8.webp 400w,
/post/rl_lessons/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_b7bf57980ec21268496775759909e319.webp 760w,
/post/rl_lessons/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_1200x1200_fit_q75_h2_lanczos_3.webp 1200w" src=/post/rl_lessons/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_c1d09df98d6fcc1d90dd947d5349c4c8.webp width=760 height=263 loading=lazy data-zoomable></div></div></figure></p><p>This shows five training runs for my <a href=https://www.jeremiahcoholich.com/publication/quadruped_footsteps/ target=_blank rel=noopener>quadruped project</a>. The desired KL-divergence was set to 0.01. The lr usually hovers around 7e-4.</p><p>Code examples for linear lr decay:</p><ul><li><a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/utils.py#L46 target=_blank rel=noopener>https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/utils.py#L46</a></li><li><a href=https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html target=_blank rel=noopener>https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html</a></li></ul><p>Code for adaptive lr:</p><ul><li><a href=https://github.com/Denys88/rl_games/blob/50d9a460f8ba41de5dbac4abed04f8de9b849f4f/rl_games/common/schedulers.py#L19 target=_blank rel=noopener>https://github.com/Denys88/rl_games/blob/50d9a460f8ba41de5dbac4abed04f8de9b849f4f/rl_games/common/schedulers.py#L19</a></li></ul><h3 id=bootstrapping-good-terminations>bootstrapping good terminations</h3><p>I have found that this is not stricly necessary (afaik, the rl_games library does without it) and can sometimes hurt (excess bootstrapping can sometimes hurt, which is a hypothesized reason that DQN and td-learning doesn&rsquo;t do that well).</p><p>This idea is this</p>$$ e^{i \pi}$$<h3 id=generalized-advantage-estimation>Generalized Advantage Estimation</h3><p>I have found GAE is useful in improving performance. Its just another knob to turn. In most training runs, I set gamma to 0.99 and lambda = 0.95. The lambda parameters can be though of as a factor used control the amount of bootstrapping. 0 is no bootstrapping, where 1 is td-learning (lots of bootstrapping). As you increase the number of samples per iteration, you will perform better with lambda set to 1.</p><p><a href=https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/storage.py#L73 target=_blank rel=noopener>https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/storage.py#L73</a></p><h3 id=entropy-decay>Entropy Decay</h3><p>The idea is simple: in the beginning you want more exploration. Towards the end you want more exploitation. This can help your agent avoid local optima early in training.
In on-policy algorithms like TRPO and PPO, you have an entropy coeffient in the loss function for your policy net. You can just decrease this entropy coefficient. In off policy algorithms like DDPG, SAC, or TD3, you have exploration during data collection and you can just decrease the variance of the distribution of the noise you add to policy output for exploration.</p><p>Actually, I have often found this is uncessary, and don&rsquo;t really use it. For my work in legged locomotion, I found that an entropy coeffient of 0 works best (perhaps because the dynamics are chaotic enough that extra exploratio noise is not necesary.)</p></div><div class="media author-card content-widget-hr"><a href=https://jmcoholich.github.io><img class="avatar mr-3 avatar-circle" src=/authors/admin/avatar_hu6b7664f523075193f9f11d79c1c9dcfa_924364_270x270_fill_q75_lanczos_center.jpg alt="Jeremiah Coholich"></a><div class=media-body><h5 class=card-title><a href=https://jmcoholich.github.io>Jeremiah Coholich</a></h5><h6 class=card-subtitle>Robotics PhD Student</h6><p class=card-text>My research interests include deep learning, reinforcement learning, and legged robots.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:jcoholich@gatech.edu><i class="fas fa-envelope"></i></a></li><li><a href="https://scholar.google.com/citations?user=KXw8YxAAAAAJ&hl=en&oi=ao" target=_blank rel=noopener><i class="fas fa-graduation-cap"></i></a></li><li><a href=https://github.com/jmcoholich target=_blank rel=noopener><i class="fab fa-github"></i></a></li><li><a href=https://www.linkedin.com/in/jeremiah-coholich/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a href=/uploads/CV_Fall_2022.pdf><i class="ai ai-cv"></i></a></li></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class="powered-by copyright-license-text">© 2022 Jeremiah Coholich</p><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-themes target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><script src=/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/latex.min.js crossorigin=anonymous></script>
<script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin=anonymous></script>
<script id=page-data type=application/json>{"use_headroom":true}</script><script src=/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js type=module></script>
<script src=/en/js/wowchemy.min.43c8f7a4851160885a7d8069dfa86538.js></script><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src=/js/wowchemy-publication.68f8d7090562ca65fc6d3cb3f8f2d2cb.js type=module></script></body></html>