<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jeremiah Coholich</title><link>https://jmcoholich.github.io/</link><atom:link href="https://jmcoholich.github.io/index.xml" rel="self" type="application/rss+xml"/><description>Jeremiah Coholich</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 29 Jun 2025 00:00:00 +0000</lastBuildDate><image><url>https://jmcoholich.github.io/media/icon_huc310f1ac1642ff51805cd109dc789c7d_32737_512x512_fill_lanczos_center_3.png</url><title>Jeremiah Coholich</title><link>https://jmcoholich.github.io/</link></image><item><title>FoundationPose and LangSAM for Robotics</title><link>https://jmcoholich.github.io/post/foundationpose/</link><pubDate>Sun, 29 Jun 2025 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/post/foundationpose/</guid><description>&lt;p>This blog post is about my experience using &lt;a href="https://nvlabs.github.io/FoundationPose/" target="_blank" rel="noopener">FoundationPose&lt;/a> with &lt;a href="https://github.com/luca-medeiros/lang-segment-anything" target="_blank" rel="noopener">LangSAM&lt;/a>. It is meant to provide advice for others and qualitative third-party results for reference.&lt;/p>
&lt;p>&lt;strong>TLDR;&lt;/strong> FoundationPose plus LangSAM works somewhat well off-the-shelf, but struggles significantly with small objects and occlusion. I was able to greatly improve results on some tasks by adding a temporal consistency score and some manual annotations to the data. The model/code has no built-in way of dealing with objects going out-of-frame or complete occlusion, which is a big practical limitation. We are still looking for ways to improve results (including switching to different pose estimation models). We only provide qualitative results videos for comparision, since we don&amp;rsquo;t have ground-truth object states in the real world to compare against. Also, see the &lt;a href="#conclusion">Conclusion&lt;/a>.&lt;/p>
&lt;!-- Object state is an important property required for many robot planning methods. In simulation, this is readily available, but in the real world it must be measured or estimated. In our case, we were working on a data augmentation method that required object pose. I decided to use FoundationPose based on the recommendation of some of my colleagues. This was my first time using a pose tracking model, as they have recently only become good. Previously, I've used [AprilTags](https://april.eecs.umich.edu/software/apriltag) to track object poses. -->
&lt;h1 id="foundationpose-overview">FoundationPose Overview&lt;/h1>
&lt;p>FoundationPose is a 6D object pose estimation model. It is trained on synthetically-augmented &lt;a href="https://objaverse.allenai.org/" target="_blank" rel="noopener">Objaverse&lt;/a> objects. At inference time, the model generates multiple pose hypotheses, ranks them, and outputs the rank 0 pose estimate. Unlike previous works, FoundationPose does not need to build a NeRF of the object first.&lt;/p>
&lt;p>FoundationPose operates either in a model-based or model-free mode, where &amp;ldquo;model&amp;rdquo; refers to a CAD model (3D mesh) of the tracked object. To run FoundationPose model-free, several reference images of the object need to be supplied. We only tried the model-based version of FoundationPose.&lt;/p>
&lt;p>For more details, see the &lt;a href="https://arxiv.org/abs/2312.08344" target="_blank" rel="noopener">paper&lt;/a>, however an in-depth understanding of FoundationPose is not required to use the model. Below is an overview of their method (Figure 2 from the paper).&lt;/p>
&lt;!-- ![FoundationPose Figure 2](FounationPose_Fig2.jpg) -->
&lt;figure id="figure-foundationpose-method-overview-source-httpsnvlabsgithubiofoundationpose">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="FoundationPose method overview (Source: https://nvlabs.github.io/FoundationPose/)" srcset="
/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_8d0d7a45685e0683d002ab6dde0be991.webp 400w,
/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_25d3bae26eff8106401da62af9a4c926.webp 760w,
/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://jmcoholich.github.io/post/foundationpose/FounationPose_Fig2_hu616dd80d5c866b1bb979f6032b3e916e_3171592_8d0d7a45685e0683d002ab6dde0be991.webp"
width="760"
height="488"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
FoundationPose method overview (Source: &lt;a href="https://nvlabs.github.io/FoundationPose/" target="_blank" rel="noopener">https://nvlabs.github.io/FoundationPose/&lt;/a>)
&lt;/figcaption>&lt;/figure>
&lt;h1 id="langsam-overview">LangSAM Overview&lt;/h1>
&lt;p>LangSAM is not a new method or architecture, but actually just code which combines the &lt;a href="https://ai.meta.com/sam2/" target="_blank" rel="noopener">Segment Anything&lt;/a> (SAM) model from Meta with the &lt;a href="https://arxiv.org/abs/2303.05499" target="_blank" rel="noopener">Grounding DINO&lt;/a> open-world object detector. Here, an understanding of what is really going on is helpful for effectively using and modifying LangSAM.&lt;/p>
&lt;p>SAM is a powerful segmentation model that can generate pixelwise segmentations for anything in an image (as the name implies). SAM takes an image and a prompt and outputs a segmentation mask. The prompt can either be points, a bounding box, or a text string. However, Meta has not released a version of SAM with text conditioning. Fortunately, this capability can be reproduced by adding Grounding DINO.&lt;/p>
&lt;p>
&lt;figure id="figure-an-overview-of-the-segment-anything-model-source-httpsarxivorgabs230402643">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="An overview of the Segment Anything model (Source: https://arxiv.org/abs/2304.02643)" srcset="
/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_c653aac0753229e743381931be70b682.webp 400w,
/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_f1a21f4b73641caa8ff8b40d2d93eabb.webp 760w,
/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://jmcoholich.github.io/post/foundationpose/sam_overview_hu74eb6c96772fc2d13df7c8e702e21e43_3218960_c653aac0753229e743381931be70b682.webp"
width="760"
height="155"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
An overview of the Segment Anything model (Source: &lt;a href="https://arxiv.org/abs/2304.02643" target="_blank" rel="noopener">https://arxiv.org/abs/2304.02643&lt;/a>)
&lt;/figcaption>&lt;/figure>
&lt;figure id="figure-example-images-segmented-by-sam-containing-400-to-500-masks-per-image-source-httpsarxivorgabs230402643">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Example images segmented by SAM containing 400 to 500 masks per image (Source: https://arxiv.org/abs/2304.02643)" srcset="
/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_8d4564c3de56d3457bd2ee272db29380.webp 400w,
/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_8cc8c342f01e7aabae97e72b716d7714.webp 760w,
/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://jmcoholich.github.io/post/foundationpose/sam_examples_hudf5507930866b1187c632823278d15d6_5304944_8d4564c3de56d3457bd2ee272db29380.webp"
width="760"
height="136"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Example images segmented by SAM containing 400 to 500 masks per image (Source: &lt;a href="https://arxiv.org/abs/2304.02643" target="_blank" rel="noopener">https://arxiv.org/abs/2304.02643&lt;/a>)
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>&lt;a href="https://arxiv.org/abs/2303.05499" target="_blank" rel="noopener">Grounding DINO&lt;/a> is an open-world object detector that takes a string of text and outputs bounding box proposals. It was created by fusing a closed-set object detector, &lt;a href="https://arxiv.org/abs/2203.03605" target="_blank" rel="noopener">DINO&lt;/a>, with a text encoder, &lt;a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT&lt;/a>. LangSAM takes the bounding box proposals from Grounding DINO and feeds them into SAM to obtain a pixel-wise segmentation mask. This &lt;a href="https://lightning.ai/blog/lang-segment-anything-object-detection-and-segmentation-with-text-prompt" target="_blank" rel="noopener">blog post&lt;/a> explains LangSAM in much more detail.&lt;/p>
&lt;p>We have two reasons for using LangSAM:&lt;/p>
&lt;ul>
&lt;li>FoundationPose requires a segmentation mask of the tracked object(s) in the first frame to initialize pose estimation.&lt;/li>
&lt;li>We want a database of object segmentations for a data-augmentation task not covered in this article.&lt;/li>
&lt;/ul>
&lt;h1 id="off-the-shelf-performance">&amp;ldquo;Off-the-shelf&amp;rdquo; Performance&lt;/h1>
&lt;p>FoundationPose requires RGBD video frames, a CAD model, camera intrinsics, and a binary segmentation mask of the tracked object in the first frame.&lt;/p>
&lt;p>Below is a visualization of our input video. Its is a VR-teleoperated demonstration of a block-stacking task.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/8bc508QxUwo" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>The three views are captured with RealSense D435 cameras running at 1280x720 resolution. The intrisic matrix is:&lt;/p>
&lt;p>$$ K = \begin{bmatrix} 912.0 &amp;amp; 0.0 &amp;amp; 640.0 \\ 0.0 &amp;amp; 912.0 &amp;amp; 360.0 \\ 0.0 &amp;amp; 0.0 &amp;amp; 1.0 \end{bmatrix} $$&lt;/p>
&lt;p>The cups, blocks, and plates we used for experiments were purchased on Amazon:&lt;/p>
&lt;ul>
&lt;li>Cups: &lt;a href="https://a.co/d/9PSu2UX" target="_blank" rel="noopener">https://a.co/d/9PSu2UX&lt;/a>&lt;/li>
&lt;li>Blocks (painted after purchasing): &lt;a href="https://a.co/d/i5k3pBq" target="_blank" rel="noopener">https://a.co/d/i5k3pBq&lt;/a>&lt;/li>
&lt;li>Plates: &lt;a href="https://a.co/d/6hOiS2a" target="_blank" rel="noopener">https://a.co/d/6hOiS2a&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>(These are not affiliate links; I&amp;rsquo;m not trying to sell anything. These are simply the objects that correspond to the CAD models I made.)&lt;/p>
&lt;p>The CAD models I created for each of them are available &lt;a href="https://github.com/jmcoholich/FoundationPose/tree/main/meshes" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>To output 6D object poses from a video frame, FoundationPose first requires an initial for both translation and rotation. The initial guess is then refined for a set number of iterations (default is 5) before a final estimate is output. For every frame except the first one, the pose estimate of the previous frame is used for initialization. When processing the first frame, FoundationPose generate 240 initial rotation guesses through sampling points and rotations on an &lt;a href="https://en.wikipedia.org/wiki/Geodesic_polyhedron" target="_blank" rel="noopener">icosphere&lt;/a>. The first-frame initial guess for translation is simply obtained through the segmentation mask.&lt;/p>
&lt;!-- For each frame of the video, FoundationPose requires an intial guess of the 6D object pose which is then iteratively refined. The FoundationPose code simply uses the final pose estimate from the previous frame as the initial guess, with the exception of the first frame. The initial guess for the first frame's object translation must be supplied by the user in the form of a segmentation mask. FoundationPose . -->
&lt;!-- FoundationPose requires the segmentation mask to intilize the translation estimates for the first video frame. Here is the mask. After the first frame, the previous frame is used. Rotation estimates are initialized by randomly sampling a sphere then refining estimates. -->
&lt;!-- ### First result
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/Gs-hkQBOIac" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
This was my first try tracking just the blue cube on a 256x256 input. The tracking fails during manipulation and the model switches to the green cube instead. The cubes are both the exact same dimensions, so it makes sense that tracking might fail when they are near each other. -->
&lt;!--
### Scene adjustments
We changed our demo setup, increased resolution of cameras to and moved cameras closer. Results were much better. Here is a visualization of our final setup.
Below are the results running FoundationPose on three camera views cropped to 720x720. Its clear that higher resolution input and relatively larger objects helps. The tracking still fails in the third camera, which is farther from the table.
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/hU97Uwc44Uc" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
The model does much better, but still fails on the third camera which is much farther away.
Still fails on this: -->
&lt;p>I modified the FoundationPose code to track all three cubes at once with a simple &amp;ldquo;for&amp;rdquo; loop. Below are the first results. I&amp;rsquo;m only running tracking on the front camera view.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CTuzFU3Y9gI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Clearly there are some issues &amp;ndash; the model is unable to track the blocks once they are moved.&lt;/p>
&lt;h1 id="foundationpose--mask-temporal-consistency">FoundationPose + Mask Temporal Consistency&lt;/h1>
&lt;p>My first idea for improving these results was to condition the pose estimate for every frame on a segmentation masks from LangSAM (instead of just the first frame). Essentially, this offloads the challenge of object localization in 2D from FoundationPose to LangSAM. However, LangSAM doesn&amp;rsquo;t work perfectly either. Using the same prompts for the same objects on every frame (&amp;ldquo;blue cube&amp;rdquo;, &amp;ldquo;red cube&amp;rdquo;, and &amp;ldquo;green cube&amp;rdquo;), this is what we get:&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/2YxygrxXshY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Watch the top right view. When the red cube is manipulated, LangSAM switches to segmenting the blue cube, then the green cube, and eventually the entire stack of cubes. However, all the segmentations in the first frame are correct, likely because none of the cubes are occluded by the gripper or stacked.&lt;/p>
&lt;p>As explained previously, bounding boxes from Grounding DINO are used to prompt SAM. For each frame, Grounding DINO outputs multiple box proposals that are above a settable threshold for bounding-box validity and text alignment then selects the box with the highest text alignment. Below is an image of the bounding box proposals and scores for the first frame for the prompt &amp;ldquo;red cube&amp;rdquo;. The highest scoring box is colored blue&lt;/p>
&lt;figure id="figure-bounding-box-proposals--and-alignment-scores-for-prompt-red-cube-from-grounding-dino">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Bounding box proposals and alignment scores for prompt &amp;#34;red cube&amp;#34; from Grounding DINO" srcset="
/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_7dc6b7d8b1ddc044b3ab9cb2a14a9ad8.webp 400w,
/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_a17385909cf543f39efbccdd02b8c98e.webp 760w,
/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://jmcoholich.github.io/post/foundationpose/GDINO_alignment_scores_hu8d0e34d67400513878995f0c10df987e_95738_7dc6b7d8b1ddc044b3ab9cb2a14a9ad8.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Bounding box proposals and alignment scores for prompt &amp;ldquo;red cube&amp;rdquo; from Grounding DINO
&lt;/figcaption>&lt;/figure>
&lt;p>I created a new scoring function to reward consistency with the previous frame. This scoring function is used instead of the prompt-alignment score for all frames except for the first one. This function is:&lt;/p>
&lt;p>$$
x_t = \arg\min_{\mathbf{x}} \left|\mathbf{x}_{t - 1} - \mathbf{x}\right|_1
$$&lt;/p>
&lt;p>Where $\mathbf{x}$ is a vector representing a bounding box.&lt;/p>
&lt;p>Or with Pytorch:&lt;/p>
&lt;pre style="font-size: 16px;color: rgb(17,179,33);background-color: black;">
dist_score = -torch.sum(torch.abs(bbox - last_bbox))
&lt;/pre>
&lt;p>We also lower the &amp;ldquo;box_threshold&amp;rdquo; and &amp;ldquo;text_threshold&amp;rdquo; from 0.3 and 0.25 to 0.1 and 0.1. This means Grounded DINO will output more bounding box proposals for us to select from.&lt;/p>
&lt;p>Here is the second frame, with the temporal consistency scores displayed:
&lt;figure id="figure-bounding-box-proposals-and-temporal-consistency-scores-for-prompt-red-cube-from-grounding-dino">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Bounding box proposals and temporal consistency scores for prompt &amp;#34;red cube&amp;#34; from Grounding DINO" srcset="
/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_8a22573388f1d3816a51a2014b20a79a.webp 400w,
/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_45285f9935203566b35bedcfe8d5355a.webp 760w,
/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://jmcoholich.github.io/post/foundationpose/temporal_consistency_scores_hu33570ce41983d67a6661a7e22086e333_78338_8a22573388f1d3816a51a2014b20a79a.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Bounding box proposals and temporal consistency scores for prompt &amp;ldquo;red cube&amp;rdquo; from Grounding DINO
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>Here is another example from the side camera view (145th frame):
&lt;figure id="figure-bounding-box-proposals-and-temporal-consistency-scores-for-prompt-red-cube-from-grounding-dino-with-many-bounding-box-proposals">
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="Bounding box proposals and temporal consistency scores for prompt &amp;#34;red cube&amp;#34; from Grounding DINO, with many bounding box proposals" srcset="
/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_f96dd70071f7f3975af0d0a7497d61e5.webp 400w,
/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_ca27947d5e14d2ce86e1762964aae5de.webp 760w,
/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_1200x1200_fit_q75_h2_lanczos.webp 1200w"
src="https://jmcoholich.github.io/post/foundationpose/temporal_consistency_scores_2_hud7d3e20a5cc566ec54feb3963b76164f_79328_f96dd70071f7f3975af0d0a7497d61e5.webp"
width="760"
height="428"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;figcaption>
Bounding box proposals and temporal consistency scores for prompt &amp;ldquo;red cube&amp;rdquo; from Grounding DINO, with many bounding box proposals
&lt;/figcaption>&lt;/figure>&lt;/p>
&lt;p>Below are the segmentations for the same demo obtained with the temporal-consistency scoring function.
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/i0zaNuNY9RM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
Clearly, the tracking is much better now. Watching the top right view again and observe that the red block is segmented correctly the entire video, even during manipulation and stacking. However, there are still some small errors. For example, for cam 0 &amp;ldquo;Franka robot arm&amp;rdquo;, the block is segmented instead of the robot (the tip of the gripper). This will be addressed in the next section.&lt;/p>
&lt;p>Below is the video of the FoundationPose results where every frame is conditioned on the improved, temporally-consistency segmentations from LangSAM.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/NemeM3IC1gU" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;p>Now, the model is able to track each block throughout the demo.&lt;/p>
&lt;p>You may also notice that the coordinate systems for each box are now aligned, in contrast to the first FoundationPose video. I realized that since cubes have many axes of symmetry, the icosphere of pose initializations lead to random cube orientations. I reduced the number of initial guesses to a single identity matrix which results in consistent cube pose estimates and a ~150% speedup of FoundationPose.&lt;/p>
&lt;h1 id="foundationpose--labels">FoundationPose + Labels&lt;/h1>
&lt;p>Occasionally, LangSAM segmentations are wrong even for the first frame, meaning the temporal consistency score only enforces an incorrect object segmentation. Additionally, sometimes the objects are occluded completely or leave the image entirely, making tracking impossible. Since we were shooting for a deadline and needed results quickly, I decided to manually add some labels to our collection of 60 demonstrations. With the help of ChatGPT, I wrote a labeling pipeline to step through each video and provide labels for object bounding boxes and &amp;ldquo;stop tracking&amp;rdquo; for select frames in the video.&lt;/p>
&lt;!-- Here are the segmentations for cups with labels. They results are the same, except now blah blah blah. -->
&lt;p>Here are the segmentations for the same stack-blocks demo again. Now, the cam 0 &amp;ldquo;Franka robot arm&amp;rdquo; segmentation is correct, even in the first frame.&lt;/p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/TDYzbGJ2REg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;h3 id="stack-cups-and-stack-plates-tasks">Stack Cups and Stack Plates Tasks&lt;/h3>
&lt;p>With the stack-blocks task working, I ran the pipeline on two new tasks &amp;ndash; stack-cups and stack-plates.&lt;/p>
&lt;p>Here is FoundationPose on stack-plates without temporal consistency and labels:&lt;/p>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/rUjEtP8KPmw" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
Tracking of the plates fails when they are lifted out of the scene and when they occlude each other on the rack.&lt;/p>
&lt;p>Here are the results after adding temporal consistency and labels:&lt;/p>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/ilF_YeErRAM" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
The labels that tell FoundationPose when to stop and reinitialize tracking significantly improves the pose estimates.&lt;/p>
&lt;!-- And for the two other tasks:
Stack cups:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/CHPH6GWDsuE" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
Stack plates:
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/ndTsDA_uEug" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
-->
&lt;p>Here is the before and after for the stack-cups task.
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/3QjOZKr2tlg" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
&lt;/p>
&lt;p>
&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
&lt;iframe src="https://www.youtube.com/embed/G7xddmpxsf4" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video">&lt;/iframe>
&lt;/div>
Much less of a difference is made here, partially because the tracking worked well off-the-shelf, for some reason. I don&amp;rsquo;t know why the model succeeded here and failed on the cubes, since both tasks are quite similar and use similarly-sized objects. Perhaps FoundationPose training dataset contained more objects like the cups.&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>Temporal consistency scoring greatly improved results for the stack-blocks task, while data annotations greatly improved results for the stack-plates task. However, the results are still far from perfect and providing high-quality annotations for demo videos tracking multiple objects is not scalable.&lt;/p>
&lt;p>We are still looking for solutions for real world object pose estimation. I talked to two other PhD students working on manipulation who had also tried to use FoundationPose and abandoned it, saying my video results were better than theirs even. For those who are interested in going down the same path, here are some recommendations:&lt;/p>
&lt;h3 id="my-recommendations">My Recommendations&lt;/h3>
&lt;p>For real-world object pose tracking, the best thing to use is motion capture. The second best thing would be to use several &lt;a href="https://april.eecs.umich.edu/software/apriltag" target="_blank" rel="noopener">AprilTags&lt;/a> on each object. If that is not an option either, then use FoundationPose or BundleSDF. To obtain decent results with these models:&lt;/p>
&lt;ul>
&lt;li>Track large objects or move the cameras closer to the scene&lt;/li>
&lt;li>Use the highest camera resolution&lt;/li>
&lt;li>Avoid occluding the tracked objects&lt;/li>
&lt;li>Avoid moving the tracked objects out of frame&lt;/li>
&lt;li>If you have a higher compute budget, try &lt;a href="https://bundlesdf.github.io/" target="_blank" rel="noopener">BundleSDF&lt;/a>. BundleSDF takes longer to run for each video, but handles occlusions much better.&lt;/li>
&lt;/ul>
&lt;p>For the LangSAM language-to-segmentation pipeline, I recommend:&lt;/p>
&lt;ul>
&lt;li>Choosing objects that are distinct from other objects in the scene in terms of shape and color&lt;/li>
&lt;li>Spending time experimenting with different prompts&lt;/li>
&lt;li>Using an LLM to generate prompts to try&lt;/li>
&lt;li>Use bounding box prompting for SAM (instead of points)&lt;/li>
&lt;/ul>
&lt;p>Here is my Fork of FoundationPose: &lt;a href="https://github.com/jmcoholich/FoundationPose" target="_blank" rel="noopener">https://github.com/jmcoholich/FoundationPose&lt;/a>.
Tips for running FoundationPose from the authors: &lt;a href="https://github.com/NVlabs/FoundationPose/issues/44#issuecomment-2048141043" target="_blank" rel="noopener">https://github.com/NVlabs/FoundationPose/issues/44#issuecomment-2048141043&lt;/a>
&lt;a href="https://github.com/030422Lee/FoundationPose_manual" target="_blank" rel="noopener">https://github.com/030422Lee/FoundationPose_manual&lt;/a>&lt;/p>
&lt;p>A huge thanks to the FoundationPose authors for developing and releasing this model! Also thank you to &lt;a href="https://www.linkedin.com/in/justin-wit/" target="_blank" rel="noopener">Justin Wit&lt;/a> for helping me setup tasks and collect data for all experiments shown.&lt;/p>
&lt;h2 id="references">References&lt;/h2>
&lt;div style="font-size: 12px">
&lt;blockquote style="margin: 0.3em 0;">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. "Bert: Pre-training of deep bidirectional transformers for language understanding." In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 4171-4186. 2019.
&lt;/blockquote>
&lt;blockquote style="margin: 0.3em 0;">
Kirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao et al. "Segment anything." In Proceedings of the IEEE/CVF international conference on computer vision, pp. 4015-4026. 2023.
&lt;/blockquote>
&lt;blockquote style="margin: 0.3em 0;">
Liu, Shilong, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang et al. "Grounding dino: Marrying dino with grounded pre-training for open-set object detection." In European Conference on Computer Vision, pp. 38-55. Cham: Springer Nature Switzerland, 2024.
&lt;/blockquote>
&lt;blockquote style="margin: 0.3em 0;">
Medeiros, Luca. 2023. lang-segment-anything. GitHub. https://github.com/luca-medeiros/lang-segment-anything.
&lt;/blockquote>
&lt;blockquote style="margin: 0.3em 0;">
Ravi, Nikhila, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr et al. "Sam 2: Segment anything in images and videos." arXiv preprint arXiv:2408.00714 (2024).
&lt;/blockquote>
&lt;blockquote style="margin: 0.3em 0;">
Wen, Bowen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Müller, Alex Evans, Dieter Fox, Jan Kautz, and Stan Birchfield. "Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 606-617. 2023.
&lt;/blockquote>
&lt;blockquote style="margin: 0.3em 0;">
Wen, Bowen, Wei Yang, Jan Kautz, and Stan Birchfield. "Foundationpose: Unified 6d pose estimation and tracking of novel objects." In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17868-17879. 2024.
&lt;/blockquote>
&lt;blockquote style="margin: 0.3em 0;">
Zhang, Hao, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M. Ni, and Heung-Yeung Shum. "Dino: Detr with improved denoising anchor boxes for end-to-end object detection." arXiv preprint arXiv:2203.03605 (2022).
&lt;/blockquote>
&lt;/div></description></item><item><title>Sim2real Image Translation Enables ViewpointRobust Policies from Fixed-Camera Datasets</title><link>https://jmcoholich.github.io/publication/iros_2025/</link><pubDate>Fri, 02 May 2025 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/iros_2025/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Hierarchical Reinforcement Learning and Value Optimization for Challenging Quadruped Locomotion</title><link>https://jmcoholich.github.io/publication/hrl_optim/</link><pubDate>Fri, 25 Apr 2025 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/hrl_optim/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Improving the Student-Teacher Approach for Semi-Supervised Semantic Segmentation</title><link>https://jmcoholich.github.io/publication/cs_8803_mean_teacher/</link><pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/cs_8803_mean_teacher/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>A Bag of Tricks for Deep Reinforcement Learning</title><link>https://jmcoholich.github.io/post/rl_bag_of_tricks/</link><pubDate>Sat, 17 Sep 2022 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/post/rl_bag_of_tricks/</guid><description>&lt;!-- This article is not an introduction for reinforcement learning and assumes you know what it is and are trying to get into it. -->
&lt;!-- Outline of this post
1. RL is a very exciting and promising field
2. BUT its hard to reproduce results and hard to apply to new fields
3. This blog post gives a list of tricks and lessons learned for beginners trying to write RL algorithms from scratch and/or apply RL algorithms to new tasks -->
&lt;!--
The list of tricks -->
&lt;!--
First of all, it is difficult to reproduce results in deep reinforcement learning ([Wired article](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/)). -->
&lt;!-- Reinforcement Learning is a category in machine learning that doesn't quite fall under the scope of supervised or unsupervised learning -->
&lt;!-- *Update 9/15/2022: Experimental results coming soon!* -->
&lt;p>When I first started studying reinforcement learning (RL), I implemented &lt;a href="https://arxiv.org/abs/1707.06347" target="_blank" rel="noopener">Proximal Policy Optimization (PPO)&lt;/a> from scratch using only the &lt;a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html#pseudocode" target="_blank" rel="noopener">psuedocode&lt;/a> on OpenAI&amp;rsquo;s website. It didn&amp;rsquo;t work and failed to obtain nearly any reward on most OpenAI Gym environments. It took a few more months of debugging, reading other RL implementations, and talking to colleagues to get things working. My conversations with other Georgia Tech students revealed that initially struggling to do basic things with RL was not uncommon. &lt;a href="https://www.alexirpan.com/2018/02/14/rl-hard.html#:~:text=Often%2C%20it%20doesn%27t%2C,out%20of%20the%20RL%20algorithm." target="_blank" rel="noopener">These&lt;/a> blog &lt;a href="https://andyljones.com/posts/rl-debugging.html" target="_blank" rel="noopener">posts&lt;/a> do a great job of explaining the difficulty with RL and really resonate with my own experiences.&lt;/p>
&lt;!-- Some of these "tricks" are will be obvious if you have experience in supervised learning, such as gradient clipping and input normalization. -->
&lt;!-- These are all things that are very important to getting this working, but are mundane enough that most of the time no one really tells you explicitly to make sure to do these things or they assume that you already know. -->
&lt;!-- Where possible, I have tried to include links to code in RL implementations where these tricks are found. I will additionally include a link to any help Pytorch functions for implementation. -->
&lt;p>In hindsight, there was no single major flaw with my initial PPO implementation, but rather many small tricks and optimizations that were missing. The purpose of this post is to enumerate these tricks and provide references to code where they are implemented. They are roughly ordered in descending order of importance. Knowledge of some of these tricks is only necessary if you are implementing an RL algorithm from scratch, as most public implementations will already include them. However, knowing of their existence will enable you to debug more effectively and make changes more intelligently.&lt;/p>
&lt;p>Different RL implementations will include a slightly different set of tricks. As evidence of their importance, check out this figure (below) from the paper &lt;a href="https://arxiv.org/pdf/1709.06560.pdf" target="_blank" rel="noopener">Deep Reinforcement Learning that Matters&lt;/a>. The authors show empirically that different popular implementions of the same RL algorithm differ significantly in performance on standard RL benchmarks, even when controlling for hyperparameters and network architecture.&lt;/p>
&lt;!-- (They also expose a similar sensitivity to hyperparameters, network architecture, random seed, reward scale, and choice of environment -- RL is very finicky). -->
&lt;!-- Different implementations include different sets of tricks -- and they really do make a difference. In -->
&lt;!-- Through the entire process, I learned of a variety of small tricks and optimizations that are typically used to get RL algorithms working on complex environments. -->
&lt;!-- I believe the different set of tricks included in each implementation is the primary cause of this inconsistency. Additionally, many of these tricks introduce their own hyperparameters. -->
&lt;!-- Deep reinforcement learning (RL) is an exiciting area of study, but it can be difficult to [reproduce results](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/) in academic papers or successfully apply RL algorithms to new domains. Part of the issue is learning all the small tricks which are sometimes not disclosed and vary between impelemtations. -->
&lt;p>
&lt;img src=fig_6_drl_that_matters.png width=75%>
&lt;p>&lt;em>Figure 6 from &lt;a href="https://arxiv.org/pdf/1709.06560.pdf" target="_blank" rel="noopener">Deep Reinforcement Learning that Matters&lt;/a>, plotting the performance of different RL implementations averaged over 5 random seeds. These variations can be explained by differences in implementation and different PyTorch/TF versions.&lt;/em>&lt;/p>
&lt;/p>
&lt;!-- When I started in 2020, I had been motivated by cool results [in](https://openai.com/blog/learning-dexterity/) [robotics](https://arxiv.org/pdf/1812.11103.pdf and [video games](https://arxiv.org/pdf/1312.5602). The generality and power of deep RL algorithms seemed very promising compared to the domain-specific trajectory optimization algorithms for robotic locomotion that I had been previously studying. -->
&lt;!-- However, in doing RL research I quickly found that it was difficult to reproduce existing results or to apply RL to new tasks. -->
&lt;!-- I began studying reinforcement learning (RL) in the summer of 2020, when I joined the [Robotics Perception and Learning Lab](https://faculty.cc.gatech.edu/~zk15/). I was motivated by the how powerful and general the algorithms seemed and the results in [video games](https://arxiv.org/pdf/1312.5602) and especially in [robotics](https://arxiv.org/pdf/1812.11103.pdf) [Shadow Hand](https://openai.com/blog/learning-dexterity/). -->
&lt;!-- However, while it is trivial to clone a popular RL repository and train policies on existing benchmarks, implementing RL algorithms from scratch or succesfully applying RL to a new task or application domain is quite difficult. Reading academic papers and understand the theory behind why an algorithm works is an important part of research, but not enough for .
My issue was that even if I understood all the theory behind an RL algorithm, either from a paper or from Spinning Up, there were still many tricks required to get RL working in practice. To draw a parallel to supervised learning, it would be like understanding SGD and neural networks, but not having knowledge of batch norms or residual connections.
Altough PPO is a SOTA algorithm, implementing pseudocode directly from the PPO paper (below) will not yeild SOTA performance. You need all the other stuff. -->
&lt;p>Now for some disclaimers &amp;ndash; nearly all of my experience comes from training on-policy algorithms for continuous control, so there may be useful tips for discrete/off-policy settings that I&amp;rsquo;m missing. Also, RL is a super-hot field and perhaps some of the content in this post is already outdated. Hopefully, this blog is at least useful to someone starting out like I was. Please don&amp;rsquo;t hesitate to reach out to me if you think there is something important missing!&lt;/p>
&lt;p>Most of the examples will come from either of these two RL implementations:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games" target="_blank" rel="noopener">RL Games&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- I don't have ablation results on all of these. -->
&lt;!-- ### Using an existing RL Implementation and environment -->
&lt;p>Implementing an RL algorithm from scratch is an excellent way to learn. However, if you just need to get something working quickly, you should instead just fork a popular repo and start from there. Here are some suggestions:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://stable-baselines3.readthedocs.io/en/master/" target="_blank" rel="noopener">Stable Baselines3&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games" target="_blank" rel="noopener">RL Games&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.ray.io/en/latest/rllib/index.html" target="_blank" rel="noopener">RLlib&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- This is the main thing you should do instead of trying to code one from scratch. Take an existing implementation, play around with it, run some benchmarks. Then make a fork and start modifying the implementation for your own project. They will include their own set of tricks, and the creators have likely already tuned it a lot on RL benchmarks and provide default hyperparameter values that work decently well. -->
&lt;!-- TODO post this on the RL discord. -->
&lt;p>Contents:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#observation-normalization-and-clipping">Observation and Normalization Clipping&lt;/a>&lt;/li>
&lt;li>&lt;a href="#dense-rewards">Dense Rewards&lt;/a>&lt;/li>
&lt;li>&lt;a href="#hyperparameter-tuning">Hyperparameter Tuning&lt;/a>&lt;/li>
&lt;li>&lt;a href="#gradient-normalization-and-clipping">Gradient Normalization and Clipping&lt;/a>&lt;/li>
&lt;li>&lt;a href="#reward-normalization-and-clipping">Reward Normalization and Clipping&lt;/a>&lt;/li>
&lt;li>&lt;a href="#advantage-standardization">Advantage Standardization&lt;/a>&lt;/li>
&lt;li>&lt;a href="#bootstrapping-incomplete-episodes">Bootstrapping Incomplete Episodes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#generalized-advantage-estimation">Generalized Advantage Estimation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#entropy-decay">Entropy Decay&lt;/a>&lt;/li>
&lt;li>&lt;a href="#value-network-loss-clipping">Value Network Loss Clipping&lt;/a>&lt;/li>
&lt;li>&lt;a href="#learning-rate-scheduling">Learning Rate Scheduling&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Thanks to &lt;a href="https://www.andrewszot.com/" target="_blank" rel="noopener">Andrew Szot&lt;/a> and &lt;a href="https://www.linkedin.com/in/mathew-piotrowicz-aa4962137/" target="_blank" rel="noopener">Mathew Piotrowicz&lt;/a> for reading drafts of this and providing feedback.&lt;/p>
&lt;h3 id="observation-normalization-and-clipping">Observation Normalization and Clipping&lt;/h3>
&lt;p>In RL, the inputs to the policy and value networks are observations, which can consist of values that differ by orders of magnitude. For example, if you are learning a policy to control a robot, your observation could contain joint angles ranging from $ -\frac{\pi}{2} $ to $ \frac{\pi}{2} $ radians and a robot position coordinate that lies between 0 and 1000 meters. Normalizing the input space to eliminate this difference in scale leads to more stable training and faster convergence. This should be nothing new to those with prior experience training neural networks.&lt;/p>
&lt;p>The two most common methods for preprocessing are standardization and rescaling. Standardization refers to subtracting the mean and dividing by the standard deviation of the data so that each dimension approximates a standard normal distribution. Rescaling means mapping the data to the range $ \left[0, 1\right] $ by subtacting the min and dividing by the range. In either case, clipping should also be applied after normalization. Neural networks are bad at extrapolation, and outliers can produce unexpected outputs. In my work, observations are clipped to $[-5.0, 5.0]$ after standardization.&lt;/p>
&lt;p>In supervised learning, statistics calculated over the training set are used to normalize each sample. In RL, this isn&amp;rsquo;t possible because the dataset (consisting of interactions with the environment) is collected online and the statistics change continuously. Because of this, you need to calculate an online mean and standard deviation. Most RL codebases use an implementation of &lt;a href="https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Welford%27s_online_algorithm" target="_blank" rel="noopener">Welford&amp;rsquo;s Online Algorithm&lt;/a> like &lt;a href="https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/running_mean_std.py" target="_blank" rel="noopener">this one&lt;/a> from Stable Baselines3.&lt;/p>
&lt;p>This online approach is best when your algorithm needs to work on many different environments. However, it often causes an initial drop in performance (red circle below) as the mean and standard deviation move rapidly early in training due a small sample size and exploration.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="pic" srcset="
/post/rl_bag_of_tricks/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_cb7351224d24cbd48db808cd6bb44e05.webp 400w,
/post/rl_bag_of_tricks/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_f3c994e5b8222cfd49a9dd4f22fd932e.webp 760w,
/post/rl_bag_of_tricks/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://jmcoholich.github.io/post/rl_bag_of_tricks/obs_norm_dip_hu981de4d7338294a9cd2c2fae25e4020c_71749_cb7351224d24cbd48db808cd6bb44e05.webp"
width="760"
height="380"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>An initial drop in performance caused by normalization statistics moving faster than the policy updates.&lt;/em>&lt;/p>
&lt;!-- Do this because optimization is much more effective when the different inputs are all the same scale, speeds up learning, and leads to faster convergence. Also avoid clipping to get rid of random or unexpected outliers, and because neural networks are bad at extrapolating. -->
&lt;p>Alternatively, if you have good prior knowledge about the bounds of the observation space, you can just rescale your data to the range [-1, 1] or [0, 1], like what they do &lt;a href="https://github.com/leggedrobotics/legged_gym/blob/dd6a6892e54c4f111a203319c05da8dca9595ae1/legged_gym/envs/base/legged_robot.py#L212" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;!-- That way you avoid computing an online mean and the warmup period. This may also be more stable wrt random seed, get out of local minima easier -->
&lt;!-- Neural networks like nice smooth inputs and outputs. -->
&lt;p>&lt;strong>Note:&lt;/strong> A common bug when replaying trained policies is the failure to save and load normalization statistics. A policy network will not work during test time if the inputs are not preprocessed the same way they were during training.&lt;/p>
&lt;p>Code examples&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/06a3319d3a6af566d984aa5953b1fd7a24a8e3a4/rl_games/common/a2c_common.py#L587" target="_blank" rel="noopener">RL Games 1&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/94e55563be60f10e659428cdce7b4e0bd131d471/rl_games/algos_torch/models.py#L41" target="_blank" rel="noopener">RL Games 2&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/41332b78dfb50321c29bade65f9d244387f68a60/a2c_ppo_acktr/envs.py#L193" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- ### Simplify the action space and add prior knowedge
PMTG, foot position instead of joints to avoid learning IK, make the outputs just deltas to a expert policy
-->
&lt;h3 id="dense-rewards">Dense Rewards&lt;/h3>
&lt;!-- (dense = every timestep, smooth = varies smoothly between regions of the state space (ie gradual change vs large steps)) -->
&lt;p>This tip will only be applicable if you are applying RL to a new task where you have the freedom to specify a reward function, rather than training on standard RL benchmarks where the reward function is part of the task.&lt;/p>
&lt;p>Sparse rewards are difficult for RL algorithms to learn from. If possible, try making your reward &lt;em>dense&lt;/em>, meaning that at every timestep the agent recieves an informantive reward as a function of the current state, previous state, and action taken. For example, instead of rewarding an agent +1.0 for reaching a goal and 0.0 otherwise, try giving a reward at every timestep that is propotional to progress towards the goal. Of course, this requires some prior knowledge of what progress looks like and can limit the types of solutions that your policy discovers.&lt;/p>
&lt;p>
&lt;img src=allsteps.png width=50%>
&lt;p>&lt;em>Figure 3 from &lt;a href="https://arxiv.org/abs/2005.04323" target="_blank" rel="noopener">ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills&lt;/a> depicting the stepping-stone task&lt;/em>&lt;/p>
&lt;/p>
&lt;p>For example, in the paper &lt;a href="https://arxiv.org/abs/2005.04323" target="_blank" rel="noopener">ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills&lt;/a>, the authors train a bipedal robot to hit a series of stepping stones. A naive reward design would give +1.0 if the robot&amp;rsquo;s foot hit the center of the foot target (depicted above), and 0.0 otherwise. Instead of doing this, the authors specify a reward function of&lt;/p>
&lt;p>$$ r_{target} = k_{target}\exp(-d/k_d) $$&lt;/p>
&lt;p>where $d$ is the distance from the foot to the target, and $ k_{target}$ and $k_d$ are hyperparameters. If the robot&amp;rsquo;s foot makes any contact with the stepping stone, it receives a reward. The closer the foot is to the center of the block, the higher the reward. The authors explain:&lt;/p>
&lt;blockquote>
&lt;p>In the initial stages of training, when the character makes contact with the target, the contact location may be far away from the center. Consequently, the gradient with respect to the target reward is large due to the exponential, which encourages the policy to move the foot closer to the center in the subsequent training iterations.&lt;/p>
&lt;/blockquote>
&lt;p>Without the dense reward, there would be no reward gradient across the state space, which makes learning more difficult.&lt;/p>
&lt;h3 id="hyperparameter-tuning">Hyperparameter Tuning&lt;/h3>
&lt;p>RL is notoriously sensitive to hyperparameters and there is no one-size-fits-all for good hyperparameter values. Typically, different implementations and different applications will require different hyperparameters. Here are just a few hyperparameters that could make a difference:&lt;/p>
&lt;ul>
&lt;li>reward function term coefficients&lt;/li>
&lt;li>number of policy updates and samples per update&lt;/li>
&lt;li>learning rate&lt;/li>
&lt;li>entropy coefficient&lt;/li>
&lt;li>value coefficient&lt;/li>
&lt;li>network architecture&lt;/li>
&lt;li>batch size and number of epochs per policy update&lt;/li>
&lt;li>clipping values for gradients, rewards, values, observations, and the PPO loss&lt;/li>
&lt;/ul>
&lt;p>The good thing is &lt;a href="https://docs.wandb.ai/guides/sweeps" target="_blank" rel="noopener">Weights &amp;amp; Biases&lt;/a> has a powerful pipeline for automated, distributed hyperparameter sweeps. They support random search, grid search, and Bayesian search.&lt;/p>
&lt;h3 id="gradient-normalization-and-clipping">Gradient Normalization and Clipping&lt;/h3>
&lt;p>This is another one that could be obvious if you have a background in deep learning already. Normalizing the gradient of the value and policy networks after each backward pass can help avoid numerical overflow, exploding gradients, or destructively large parameter updates. Other tricks for avoiding these same issues include reward normalization and clipping, value function loss clipping, and advantage standardization.&lt;/p>
&lt;p>Code examples:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/8da6852f72bdbe867bf12f792b00df944b419c43/rl_games/common/a2c_common.py#L252" target="_blank" rel="noopener">RL Games&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/algo/ppo.py#L82" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html" target="_blank" rel="noopener">torch.nn.utils.clip_grad_norm_&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_value_.html" target="_blank" rel="noopener">torch.nn.utils.clip_grad_value_&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="reward-normalization-and-clipping">Reward Normalization and Clipping&lt;/h3>
&lt;p>Typically, it is best not to have reward values that differ by many orders of magnitude. For example, in the paper &lt;a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf" target="_blank" rel="noopener">Playing Atari with Deep Reinforcement Learning&lt;/a>, the authors clip all rewards to the range $ \left[-1, 1\right] $.&lt;/p>
&lt;blockquote>
&lt;p>Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude.
&lt;br>
&amp;ndash; &lt;cite>Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. &amp;ldquo;Playing atari with deep reinforcement learning.&amp;rdquo; arXiv preprint arXiv:1312.5602 (2013).&lt;/cite>&lt;/p>
&lt;/blockquote>
&lt;p>In addition to just clipping the rewards, you can also keep a running mean and standard deviations of rewards to standardize rewards or returns (discounted rewards).&lt;/p>
&lt;p>Code examples:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/model.py" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>
&lt;ul>
&lt;li>Rewards are processed by these two environment wrappers from Stable Baselines3&lt;/li>
&lt;li>&lt;a href="https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html#stable_baselines3.common.atari_wrappers.ClipRewardEnv" target="_blank" rel="noopener">stable_baselines3.common.atari_wrappers.ClipRewardEnv&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#stable_baselines3.common.vec_env.VecNormalize.normalize_reward" target="_blank" rel="noopener">stable_baselines3.common.vec_env.VecNormalize.normalize_reward&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="advantage-standardization">Advantage Standardization&lt;/h3>
&lt;p>Before calculating a loss for the policy network, advantages are computed and then standardized, such that about half of the advantages are positive and about half are negative. This is done for stability of training and variance reduction. Here is an excerpt from &lt;a href="http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/hw2_final.pdf" target="_blank" rel="noopener">HW2&lt;/a> of the Berkely Deep RL course:&lt;/p>
&lt;blockquote>
&lt;p>A trick which is known to usually boost empirical performance by lowering variance of the
estimator is to center advantages and normalize them to have mean of 0 and a standard
deviation of 1.
From a theoretical perspective, this does two things:&lt;/p>
&lt;ul>
&lt;li>Makes use of a constant baseline at all timesteps for all trajectories, which does not
change the policy gradient in expectation.&lt;/li>
&lt;li>Rescales the learning rate by a factor of 1/σ, where σ is the standard dev of the
empirical advantages.&lt;/li>
&lt;/ul>
&lt;/blockquote>
&lt;p>Code Examples&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/41332b78dfb50321c29bade65f9d244387f68a60/a2c_ppo_acktr/algo/ppo.py#L36" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/7b5f9500ee65ae0832a7d8613b019c333ecd932c/rl_games/common/a2c_common.py#L857" target="_blank" rel="noopener">RL Games&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- ### ADAM optimizer -->
&lt;h3 id="bootstrapping-incomplete-episodes">Bootstrapping Incomplete Episodes&lt;/h3>
&lt;!-- TODO: always bootstrap INCOMPLETE episodes or timeouts on continuous tasks. Timeout can be a completed episode. Does RL Games really not do this? Make sure I have my formulas right (do I need to include expectations?) -->
&lt;p>In most RL pipelines, the environment runs for a pre-specified number of steps before a policy update occurs. This means the sample collection will often end before the episode does, meaning the policy will be updated with samples from incomplete episodes. When returns (sum of discounted future rewards) are calculated, this truncation makes it seem as if the agent received zero reward for the rest of the episode. To correct this error, the return computation can be &amp;ldquo;bootstrapped&amp;rdquo; with the value estimate of the final state.&lt;/p>
&lt;!-- It is fine to perform updates like this, but learning may be slower, especially if you don't have very many samples per policy update. Bootstrapping terminal states corresponding to timeouts can increase the speed of learning. -->
&lt;p>The dictionary definition of bootstrap:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Bootstrap (verb)&lt;/strong>&lt;/p>
&lt;ol>
&lt;li>get (oneself or something) into or out of a situation using existing resources. &lt;br>
&amp;ldquo;the company is bootstrapping itself out of a marred financial past&amp;rdquo; &lt;br>&lt;/li>
&lt;/ol>
&lt;p>Source: &lt;a href="https://languages.oup.com/google-dictionary-en/" target="_blank" rel="noopener">OxfordLanguages&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>In the context of RL, bootstrapping means estimating value function or Q-function targets using estimates from the same value or Q-function (&amp;ldquo;existing resources&amp;rdquo;). Bootstrapping is done with every sample in &lt;a href="https://en.wikipedia.org/wiki/Temporal_difference_learning" target="_blank" rel="noopener">temporal difference learning&lt;/a> (TD-learning) and Q-learning. In TD learning, the value estimates are :&lt;/p>
&lt;p>$$\hat{V}(s_0) = r_0 + \gamma V(s_{1})$$&lt;/p>
&lt;!-- Which leads to the following value update.
$$V(s) \leftarrow V(s) + \alpha (r + \gamma V(s') - V(s))$$
The target value for the value function is $r + \gamma V(s') $ where $s'$ is the state after $s$. $ \alpha $ is the learning rate, and $ \gamma $ is the discount factor. -->
&lt;!-- In policy gradient methods, the objective is to -->
&lt;!-- The RL objective is to maximize the expected discounted sum of future rewards, which is approximated through samples. --> At the other end of the spectrum, values can be estimated for a state $s_0$ without bootstrapping using complete trajectories that start at $s_0$. The value estimate for a state $ s_0 $ from a single rollout from $s_0$ to $s_H$ is:
&lt;p>$$ \hat{V}(s_0) = \sum_{t = 0}^{H}\gamma^t r_t $$&lt;/p>
&lt;p>When the episode gets truncated at $ h &amp;lt; H $, we can bootstrap this calculation using the value estimate of the final state. Note how $r_h$ is discarded and replaced with $V(s_h)$.&lt;/p>
&lt;p>$$ \hat{V}(s_0) = \sum_{t = 0}^{h - 1}\gamma^t r_t + \gamma^{h}V(s_{h}) $$&lt;/p>
&lt;p>Bootstrapping can help, but it can also hurt. It reduces variance in the computation of returns at the expense of introducing a bias from the value network. Here are some excerpts from Sutton and Barto&amp;rsquo;s textbook, where they place bootstrapping in the &amp;ldquo;Deadly Triad&amp;rdquo; of instability and divergence.&lt;/p>
&lt;blockquote>
&lt;p>&amp;hellip;bootstrapping methods using function approximation may actually diverge to infinity.
&amp;hellip;Bootstrapping often results in faster learning because it allows learning to take advantage of the state property, the ability to recognize a state upon returning to it. On the other hand, bootstrapping can impair learning on problems where the state representation is poor and causes poor generalization. &lt;br>
&amp;ndash; &lt;cite> &lt;a href="https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf" target="_blank" rel="noopener">Barto, Sutton. Reinforcement Learning: An Introduction. 2018&lt;/a> &lt;/cite>&lt;/p>
&lt;/blockquote>
&lt;p>Controlling this bias-variance tradeoff with bootstrapping is a central idea in &lt;a href="#generalized-advantage-estimation">Generalized Advantage Estimation (GAE)&lt;/a>.&lt;/p>
&lt;!-- I have found that this is not stricly necessary (afaik, the rl_games library does without it) and can sometimes hurt (excess bootstrapping can sometimes hurt, which is a hypothesized reason that DQN and td-learning doesn't do that well). The returns will be the same in expectation, but suffer from higher variance. -->
&lt;!-- This idea is this
I believe this is more necessary as your number of samples per update decreases. -->
&lt;p>This kind of bootstrapping should also be applied to timeout terminations on continouous tasks. Continuous tasks are those where episodes do not end once a particular objective is achieved. One example of this is robot locomotion, where success could be foward walking that continues indefinitely. However, in order to increase sample diversity, episodes are typically subject to a timeout. Since the timeout is independent of the robot&amp;rsquo;s performance, the returns should be bootstrapped.&lt;/p>
&lt;!-- An example of a discrete task would be robotic object rearragement, where once the objects are in the goal configuration, the episode ends. Timeouts are implemented in continouous tasks for diversity of samples as opposed to setting a time limit on success. -->
&lt;!-- If your value function is mostly accurate and the values are changing slowly, bootstrapping can help. Otherwise, it can introduce instability (value function overestimation). -->
&lt;p>Code examples:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/storage.py#L86" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>
&lt;ul>
&lt;li>In this computation, the tensor &lt;code>self.bad_masks&lt;/code> indicates when bootstrapping should occur. If its value is 0, then the reward at the terminal timestep is replaced with the value estimate of the terminal state.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!-- - [RL Games](https://github.com/Denys88/rl_games/blob/d6ccfa59c85865bc04d80ca56b3b0276fec82f90/rl_games/common/a2c_common.py#L474) -->
&lt;h3 id="generalized-advantage-estimation">Generalized Advantage Estimation&lt;/h3>
&lt;!-- TODO: add stuff for bootstrapping and terminal states. -->
&lt;p>Generalized Advantage Estimation (GAE), from the paper from the paper &lt;a href="https://arxiv.org/pdf/1506.02438.pdf" target="_blank" rel="noopener">High-Dimensional Continuous Control Using Generalized Advantage Estimation&lt;/a>, provides a continuous bias-variance tradeoff through controlling the amount of bootstrapping via a parameter $\lambda$. The formula for computing advantages is given below, but I highly recommend reading the actual paper if you are going to program this yourself.&lt;/p>
$$ \hat{A}^{GAE(\gamma, \lambda)}_t = \sum^\infty_{l=0} (\gamma \lambda)^l \delta^V_{t+l} $$
$$\delta_{t}^V = r_t + \gamma V(s_{t+1}) -V(s_t)$$
&lt;p>The TD-learning value update is a special case of GAE when $\lambda = 0$. When $\lambda = 1$, no bootstrapping occurs. Most of the time, I set $\gamma = 0.99$ and $\lambda = 0.95$.&lt;/p>
&lt;p>The above two equations from the paper deal with the infinite time horizon case. Dealing with terminations and bootstrapping in the finite-horizon case can be confusing, so I&amp;rsquo;ve provided some equations below. Again, assume these are estimates of advantages and values from a single trajectory from $s_0$ to $s_H$ where truncation occurs at timestep $h &amp;lt; H$.&lt;/p>
&lt;p>&lt;strong>Finite time horizon, no bootstrapping (full episode)&lt;/strong>&lt;br>
Note that $\delta_H$ becomes $r_H - V(s_H)$ because $V(s_{H+1})$ is zero.
&lt;br>&lt;/p>
&lt;p>$$\hat{A}^{GAE(\gamma, \lambda)}_t= \sum_{t=0}^{H-1} (\gamma \lambda)^t \delta^t + (\gamma \lambda)^H (r_H - V(s_H))$$
$$\hat{V}^{GAE(\gamma, \lambda)}_t= \hat{A}^{GAE(\gamma, \lambda)}_t + V(s_o)$$&lt;/p>
&lt;br>
&lt;p>&lt;strong>Finite time horizon with bootstrapping&lt;/strong> &lt;br>
Note that $r_h$ is replaced with $V(s_h)$, which makes the final term zero.&lt;/p>
&lt;p>$$\hat{A}^{GAE(\gamma, \lambda)}_t= \sum_{t=0}^{h-1} (\gamma \lambda)^t \delta^t $$
$$\hat{V}^{GAE(\gamma, \lambda)}_t= \hat{A}^{GAE(\gamma, \lambda)}_t + V(s_o)$$&lt;/p>
&lt;p>Code examples:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/storage.py#L73" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/d6ccfa59c85865bc04d80ca56b3b0276fec82f90/rl_games/common/a2c_common.py#L474" target="_blank" rel="noopener">RL Games&lt;/a>
&lt;ul>
&lt;li>As far as I can tell, RL Games does not do any bootstrapping of truncated episodes or timeouts. No information about the nature of terminations is received from the environment, and there is no condition where the advantage of a timestep is set to zero.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="entropy-decay">Entropy Decay&lt;/h3>
&lt;p>The exploration-exploitation tradeoff is a fundamental problem in RL which is usually dealt with through experimentation or hyperparamter tuning. Generally, you want more exploration early in training. The most basic way to increase exploration is to increase the entropy of the policy used to obtain environment samples. Assuming the policy outputs to a Gaussian distribution over actions, the entropy is proportional to the log of the variance. In on-policy algorithms like TRPO and PPO, entropy can be controlled indirectly via a loss term that reward entropy. In off-policy algorithms like DDPG, SAC, or TD3, noise is added to the output of a deterministic policy during sample collection. The entropy of the sampling process can be directly controlled via this noise. Starting with a high entropy coefficient/high-variance noise and decaying the desired entropy to zero may yield the desired exploration-exploitation behavior.&lt;/p>
&lt;p>In my own work in legged locomotion, I have often found this uncessary. The majority of the time, I use PPO and set the entropy coefficient to 0.0 for the entirety of training. Perhaps the chaotic underactuated dynamics of the legged robot eliminates the need for extra exploration noise.&lt;/p>
&lt;!-- you have exploration during data collection and you can just decrease the variance of the distribution of the noise you add to policy output for exploration. -->
&lt;p>Code example:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/7b5f9500ee65ae0832a7d8613b019c333ecd932c/rl_games/common/schedulers.py#L54" target="_blank" rel="noopener">RL Games&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- One of the primary ways to control entropy is through an entropy term in the policy loss function. If your policy outputs to a Gaussian distribution over actions, the entropy loss acts on the variance of that distribution. -->
&lt;!--
You can just decrease this entropy coefficient. In off policy algorithms like DDPG, SAC, or TD3, you have exploration during data collection and you can just decrease the variance of the distribution of the noise you add to policy output for exploration. -->
&lt;h3 id="value-network-loss-clipping">Value Network Loss Clipping&lt;/h3>
&lt;p>This is another trick aimed at controlling the behavior of the gradients and preventing excessively large updates. The value function is trained on a mean-squared error (MSE) loss where the target values are value estimates from policy rollouts. This is contrast to supervised learning, where the targets are stationary ground-truth labels. Because the targets themselves are estimates derived from a stochastic sampling process, inaccurate targets which produce large errors can occur.&lt;/p>
&lt;p>Value network loss clipping roughly constrains the change in value estimates between policy iterations to a &amp;ldquo;trust region&amp;rdquo; of $\pm\ \epsilon$ from the old value estimates. (Constraining updates to a trust region is the central idea behind TRPO and PPO, but for action probabilities instead of value estimates.) The loss calculation is given below, where backpropogation happens through $V_{new}(s_0)$ only.&lt;/p>
&lt;!-- $$\text{Update if:} \ V_{new}(s_0) \in \left[V_{old}(s_0) - \epsilon, V_{old}(s_0) + \epsilon\right] $$ $$OR\ \
|V_{old} - V_{target}| &lt; |V_{old} - V_{new}|$$ -->
$$ V_{clip}(s_0) = V_{old}(s_0) + \text{Clip}(V_{new}(s_0) - V_{old}(s_0),\ - \epsilon,\ \epsilon) $$
$$\mathcal{L}_{\text{MSE-Clip}} = (V_{clip}(s_0) - V_{target}(s_0))^2 $$
$$\mathcal{L}_{\text{MSE}} = (V_{new}(s_0) - V_{target}(s_0))^2 $$
$$\mathcal{L}_{\text{final}} = \text{max}(\mathcal{L}_{\text{MSE-Clip}}, \mathcal{L}_{\text{MSE}})$$
&lt;p>$\epsilon$ is usually set to something like $0.2$. Note that $V_{new}(s_0)$ could end up slightly outside of $\left[V_{old}(s_0) - \epsilon, V_{old}(s_0) + \epsilon\right]$. This is because the values themselves are not clipped, rather, the updates to the value function stop happening when clipping occurs ($\epsilon$ is just a constant with no dependency on the value network parameters &amp;ndash; no backprop can occur through $\epsilon$). This could also be due to parameter updates from loss terms corresponding to states other than $s_0$.&lt;/p>
&lt;p>Honestly, the clipped value update is rather confusing, especially at first. In my analysis, I discovered an edge case where value updates should occur, but don&amp;rsquo;t (figure below). Moving $V_{new}$ in the direction of $V_{target}$ will move $V_{new}$ closer to the trust region, but this update doesn&amp;rsquo;t occur because the distance from the nearest edge of the trust region to $V_{target}$ is greater than the distance between $V_{target}$ and $V_{new}$. However, perhaps the clipped MSE loss makes it unlikely that $V_{new}$ will end up far outside the trust region in the first place.&lt;/p>
&lt;p>
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="pic"
src="https://jmcoholich.github.io/post/rl_bag_of_tricks/value_edge_case.svg"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>An edge case in the value network loss clipping trick, where updates to $V_{new}$ in the direction of $V_{old}$ are prevented.&lt;/em>&lt;/p>
&lt;!-- The MSE loss can be clipped from [-k, k] where k is usually around 0.2. -->
&lt;p>Strangely, I couldn&amp;rsquo;t find much mention of value network loss clipping in the academic literature or on the internet, and I don&amp;rsquo;t know if this technique goes by another name. I only found &lt;a href="https://research.google/pubs/pub50213/" target="_blank" rel="noopener">this paper&lt;/a> (&amp;ldquo;PPO-style pessimistic clipping&amp;rdquo;) and this &lt;a href="https://github.com/openai/baselines/issues/91" target="_blank" rel="noopener">GitHub issue&lt;/a>. I don&amp;rsquo;t think &amp;ldquo;pessimistic clipping&amp;rdquo; is an appropriate name, since &amp;ldquo;pessimism&amp;rdquo; in the context of value functions in RL usually means values are underestimated.&lt;/p>
&lt;p>Code Examples:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/8da6852f72bdbe867bf12f792b00df944b419c43/rl_games/common/common_losses.py#L7" target="_blank" rel="noopener">RL Games&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/41332b78dfb50321c29bade65f9d244387f68a60/a2c_ppo_acktr/algo/ppo.py#L68" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>&lt;/li>
&lt;/ul>
&lt;!-- ### PPO loss -->
&lt;!-- ### shared actor-critic layers -->
&lt;h3 id="learning-rate-scheduling">Learning Rate Scheduling&lt;/h3>
&lt;p>A linearly decreasing learning rate is a common technique for training neural networks. The idea is that in the beginning of training, the optimizer should take large steps to minimize loss rapidly, while near the end, the steps should be smaller to facilitate convergence to a local optima.&lt;/p>
&lt;p>A fancier way to control the learning rate is to adaptively set it based on a desired KL-divergence between policy iterations. In most of my work, I use the RL Games implementation of an adaptive learning rate with an initial learning rate of 1e-5. Here is what the learning rate and KL-divergence plots usually looks like:
&lt;figure >
&lt;div class="d-flex justify-content-center">
&lt;div class="w-100" >&lt;img alt="pic" srcset="
/post/rl_bag_of_tricks/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_c1d09df98d6fcc1d90dd947d5349c4c8.webp 400w,
/post/rl_bag_of_tricks/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_b7bf57980ec21268496775759909e319.webp 760w,
/post/rl_bag_of_tricks/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_1200x1200_fit_q75_h2_lanczos_3.webp 1200w"
src="https://jmcoholich.github.io/post/rl_bag_of_tricks/adaptive_lr_hu195bc64329187de28f24674117a3749b_108774_c1d09df98d6fcc1d90dd947d5349c4c8.webp"
width="760"
height="263"
loading="lazy" data-zoomable />&lt;/div>
&lt;/div>&lt;/figure>
&lt;em>KL-divergence and learning rate plotted for five training runs from my &lt;a href="https://www.jeremiahcoholich.com/publication/quadruped_footsteps/" target="_blank" rel="noopener">quadruped project&lt;/a>. The desired KL-divergence was set to 0.01. The learning rate hovers around 7e-4.&lt;/em>&lt;/p>
&lt;p>Code examples for linear learning rate decay:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/efc71f600a2dca38e188f18ca85b654b37efd9d2/a2c_ppo_acktr/utils.py#L46" target="_blank" rel="noopener">pytorch-a2c-ppo-acktr-gail&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html" target="_blank" rel="noopener">torch.optim.lr_scheduler.StepLR&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Code for adaptive learning rate:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Denys88/rl_games/blob/50d9a460f8ba41de5dbac4abed04f8de9b849f4f/rl_games/common/schedulers.py#L19" target="_blank" rel="noopener">RL Games&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Thanks for reading!&lt;/p></description></item><item><title>I Completed the Google Foobar Challenge</title><link>https://jmcoholich.github.io/post/google_foobar/</link><pubDate>Tue, 31 May 2022 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/post/google_foobar/</guid><description>&lt;!-- ## Here's what the challenge is -->
&lt;p>The Google Foobar challenge is a programming challenge used by Google for recruiting. You must be invited to participate (&lt;a href="https://bootcamp.uxdesign.cc/google-foobar-googles-secret-hiring-process-bc92f011c6ad" target="_blank" rel="noopener">article&lt;/a> with more info). The challenge contains five levels of increasing difficulty, which require knowlege in the following areas:&lt;/p>
&lt;ul>
&lt;li>dynamic programming&lt;/li>
&lt;li>graph theory&lt;/li>
&lt;li>combinatorics&lt;/li>
&lt;/ul>
&lt;p>The final level was indeed the most challenging and required me to understand &lt;a href="https://en.wikipedia.org/wiki/Permutation_group" target="_blank" rel="noopener">permutation groups&lt;/a> and apply the &lt;a href="https://en.wikipedia.org/wiki/P%C3%B3lya_enumeration_theorem" target="_blank" rel="noopener">Pólya enumeration theorem&lt;/a>.&lt;/p>
&lt;!-- ## Here's a screenshot of the finish
## Here are some concepts I had to use -->
&lt;!-- - Burnside's Lemma and Pólya enumeration theorem, group theory, permutation groups, orbits and stabilizers, group actions, Stirling numbers of the first kind, cycle indices -->
&lt;!-- The last question was the hardest and involved (the above). I had to refresh myself on group theory and learn Burnside's lemma. -->
&lt;!-- What were all the problems I had
level 1: Remove names that occur more than n times from a list
level 2: arranging plates to get numbers divisible by 3, some other DP stuff
level 3: triplets of divisible numbers (DP), the replicating bombs (discrete math, graphs?), number of possible staircases that can be built (DP)
level 4: laser gun bouncing (idk), the set cover ish thing (combinatorics) -->
&lt;p>Upon completion, I received a message encrypted in Base64, which translated to:&lt;/p>
&lt;p>{&amp;lsquo;success&amp;rsquo; : &amp;lsquo;great&amp;rsquo;,&lt;br>
&amp;lsquo;colleague&amp;rsquo; : &amp;rsquo;esteemed&amp;rsquo;,&lt;br>
&amp;rsquo;efforts&amp;rsquo; : &amp;lsquo;incredible&amp;rsquo;,&lt;br>
&amp;lsquo;achievement&amp;rsquo; : &amp;lsquo;unlocked&amp;rsquo;,&lt;br>
&amp;lsquo;rabbits&amp;rsquo; : &amp;lsquo;safe&amp;rsquo;,&lt;br>
&amp;lsquo;foo&amp;rsquo; : &amp;lsquo;win!&amp;rsquo;}&lt;/p>
&lt;p>I really enjoyed solving these problems &amp;ndash; thanks &lt;a href="https://www.linkedin.com/in/mathew-piotrowicz-aa4962137/" target="_blank" rel="noopener">Mathew&lt;/a> for the invite link!&lt;/p></description></item><item><title>Generalizing Learned Policies to Unseen Environments using Meta Strategy Optimization</title><link>https://jmcoholich.github.io/publication/dl_proj/</link><pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/dl_proj/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Sim2Real Transfer for Quadrupedal Locomotion</title><link>https://jmcoholich.github.io/publication/quals/</link><pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/quals/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Soft Foot Sensor Design and Terrain Classification for Dynamic Legged Locomotion</title><link>https://jmcoholich.github.io/publication/cassie_foot_sensor/</link><pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/cassie_foot_sensor/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Reinforcement Learning for Dynamical Systems</title><link>https://jmcoholich.github.io/publication/stat_ml_class_proj/</link><pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/stat_ml_class_proj/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Compliance Shaping for Control of Strength Amplification Exoskeletons with Elastic Cuffs</title><link>https://jmcoholich.github.io/publication/arm_exo/</link><pubDate>Mon, 08 Jul 2019 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/arm_exo/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Evaluation of energy density measures and validation for powder bed fusion of polyamide</title><link>https://jmcoholich.github.io/publication/sls_project/</link><pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/publication/sls_project/</guid><description>&lt;!-- &lt;div class="alert alert-note">
&lt;div>
Click the &lt;em>Cite&lt;/em> button above to demo the feature to enable visitors to import publication metadata into their reference management software.
&lt;/div>
&lt;/div>
&lt;div class="alert alert-note">
&lt;div>
Create your slides in Markdown - click the &lt;em>Slides&lt;/em> button to check out the example.
&lt;/div>
&lt;/div>
Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/). --></description></item><item><title>Example Project</title><link>https://jmcoholich.github.io/project/example2/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/project/example2/</guid><description>&lt;p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p>
&lt;p>Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p>
&lt;p>Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p>
&lt;p>Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p>
&lt;p>Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p></description></item><item><title>Building my own 3D printer</title><link>https://jmcoholich.github.io/project/printer/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/project/printer/</guid><description>&lt;p>During my freshman year at UT, I built a 3D printer using tools in the makerspace. I followed this tutorial: &lt;a href="https://www.instructables.com/Building-a-3D-Printer-Under-200/" target="_blank" rel="noopener">https://www.instructables.com/Building-a-3D-Printer-Under-200/&lt;/a>&lt;/p>
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/4qw9EHVgvQw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>&lt;/iframe></description></item><item><title/><link>https://jmcoholich.github.io/admin/config.yml</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://jmcoholich.github.io/admin/config.yml</guid><description/></item></channel></rss>